# VRT-Spike æ€§èƒ½ä¼˜åŒ–å®Œæ•´æŒ‡å—

> **ç»Ÿä¸€çš„æ€§èƒ½å’Œå†…å­˜ä¼˜åŒ–æ–¹æ¡ˆ**  
> æ•´åˆè‡ª: OPTIMIZATION_GUIDE.md + MEMORY_OPTIMIZATION.md  
> æœ€åæ›´æ–°: 2025-10-21

---

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿè¯Šæ–­](#å¿«é€Ÿè¯Šæ–­)
2. [å†…å­˜ä¼˜åŒ–](#å†…å­˜ä¼˜åŒ–)
3. [æ¨¡å‹æ€§èƒ½ä¼˜åŒ–](#æ¨¡å‹æ€§èƒ½ä¼˜åŒ–)
4. [DataLoaderä¼˜åŒ–](#dataloaderä¼˜åŒ–)
5. [ç»¼åˆä¼˜åŒ–æ–¹æ¡ˆ](#ç»¼åˆä¼˜åŒ–æ–¹æ¡ˆ)
6. [ç›‘æ§å’Œè°ƒè¯•](#ç›‘æ§å’Œè°ƒè¯•)
7. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ğŸš€ å¿«é€Ÿè¯Šæ–­

### 1. è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ

**ç—‡çŠ¶æ£€æŸ¥æ¸…å•**:

| ç—‡çŠ¶ | å¯èƒ½åŸå›  | è·³è½¬ç« èŠ‚ |
|------|---------|---------|
| å†…å­˜å¿«é€Ÿå¢é•¿è‡³OOM | DataLoaderå†…å­˜çˆ†ç‚¸ | [å†…å­˜ä¼˜åŒ–](#å†…å­˜ä¼˜åŒ–) |
| GPUåˆ©ç”¨ç‡ä½ (<50%) | æ•°æ®åŠ è½½ç“¶é¢ˆ | [DataLoaderä¼˜åŒ–](#dataloaderä¼˜åŒ–) |
| å‰å‘ä¼ æ’­æ…¢ (>1500ms) | æ¨¡å‹è®¡ç®—ç“¶é¢ˆ | [æ¨¡å‹æ€§èƒ½ä¼˜åŒ–](#æ¨¡å‹æ€§èƒ½ä¼˜åŒ–) |
| è®­ç»ƒé€Ÿåº¦æ…¢ (<1 sample/s) | ç»¼åˆé—®é¢˜ | [ç»¼åˆä¼˜åŒ–æ–¹æ¡ˆ](#ç»¼åˆä¼˜åŒ–æ–¹æ¡ˆ) |

### 2. æ€§èƒ½åˆ†æå·¥å…·

```bash
# è¿è¡Œæ€§èƒ½åˆ†æ
python analyze_performance.py outputs/logs/train_<timestamp>.log

# å†…å­˜è¯Šæ–­
python docs/diagnose_memory.py
```

---

## ğŸ’¾ å†…å­˜ä¼˜åŒ–

### é—®é¢˜è¯Šæ–­

#### ä¸ºä»€ä¹ˆä¼šå‡ºç°å†…å­˜çˆ†ç‚¸ï¼Ÿ

**æ ¹æœ¬åŸå› ï¼š**

1. **æ•°æ®é›†è§„æ¨¡å¤§**
   - GoPro+Spikeæ•°æ®é›†çº¦2,222å¸§
   - æ¯å¸§: blur(720Ã—1280Ã—3Ã—4) + sharp(720Ã—1280Ã—3Ã—4) + voxel(32Ã—720Ã—1280Ã—4) â‰ˆ 133.59 MB
   - æ€»æ•°æ®é‡: ~290 GB

2. **DataLoader workerå†…å­˜å¤åˆ¶**
   - æ¯ä¸ªworkerè¿›ç¨‹forkæ—¶å¤åˆ¶çˆ¶è¿›ç¨‹å†…å­˜
   - 36 workers Ã— 3 GPUs = 108ä¸ªè¿›ç¨‹
   - ç†è®ºæœ€åæƒ…å†µ: 290GB Ã— 108 â‰ˆ 31TBï¼

3. **æ— é™åˆ¶ç¼“å­˜ç­–ç•¥**
   - ç®€å•å­—å…¸ç¼“å­˜è¯•å›¾åŠ è½½æ‰€æœ‰æ•°æ®åˆ°RAM
   - æ²¡æœ‰å†…å­˜é™åˆ¶ï¼Œå¯¼è‡´çˆ†ç‚¸æ€§å¢é•¿

### è§£å†³æ–¹æ¡ˆï¼šLRUç¼“å­˜

#### 1. LRU (Least Recently Used) ç¼“å­˜

**åŸç†ï¼š**
- è®¾ç½®å†…å­˜ä¸Šé™ï¼ˆé»˜è®¤50GBï¼‰
- ç¼“å­˜æœ€è¿‘ä½¿ç”¨çš„æ•°æ®
- è¾¾åˆ°ä¸Šé™æ—¶è‡ªåŠ¨æ·˜æ±°æœ€ä¹…æœªä½¿ç”¨çš„æ•°æ®
- ä¿æŒé«˜å‘½ä¸­ç‡åŒæ—¶é˜²æ­¢å†…å­˜çˆ†ç‚¸

**å®ç°ï¼š**
```python
class LRUCache:
    def __init__(self, max_memory_gb: float = 50.0):
        self.max_memory_bytes = int(max_memory_gb * 1024**3)
        self.cache: OrderedDict[str, torch.Tensor] = OrderedDict()
        self.current_memory = 0
    
    def get(self, key: str) -> torch.Tensor | None:
        if key in self.cache:
            self.cache.move_to_end(key)  # æ ‡è®°ä¸ºæœ€è¿‘ä½¿ç”¨
            return self.cache[key].clone()
        return None
    
    def put(self, key: str, value: torch.Tensor) -> None:
        # æ·˜æ±°LRUé¡¹ç›´åˆ°æœ‰è¶³å¤Ÿç©ºé—´
        while self.current_memory + item_size > self.max_memory_bytes:
            self.cache.popitem(last=False)  # åˆ é™¤æœ€æ—§é¡¹
        self.cache[key] = value
```

#### 2. é…ç½®å‚æ•°

```yaml
DATA:
  USE_RAM_CACHE: true           # å¯ç”¨LRUç¼“å­˜ï¼ˆæ¨èï¼‰
  CACHE_SIZE_GB: 50.0           # ç¼“å­˜å¤§å°é™åˆ¶ï¼ˆGBï¼‰
  
DATALOADER:
  TOTAL_WORKERS: 12             # DataLoader workersæ•°é‡
  TRAIN_PREFETCH_FACTOR: 4      # é¢„å–æ‰¹æ¬¡æ•°
```

#### 3. å‚æ•°è°ƒä¼˜å»ºè®®

**CACHE_SIZE_GBï¼š**
- 256GBç³»ç»Ÿ: 50-80GBï¼ˆç•™è¶³ç©ºé—´ç»™æ¨¡å‹å’Œç³»ç»Ÿï¼‰
- 128GBç³»ç»Ÿ: 20-40GB
- 64GBç³»ç»Ÿ: 10-20GB

**TOTAL_WORKERSï¼š**
- å•GPU: 4-8
- å¤šGPU: æ¯GPU 4ä¸ªï¼Œæ€»è®¡ `num_gpus Ã— 4`
- æœ€å¤§ä¸è¶…è¿‡CPUæ ¸å¿ƒæ•°çš„50%

#### 4. æ€§èƒ½å¯¹æ¯”

| é…ç½® | å†…å­˜å ç”¨ | è®­ç»ƒé€Ÿåº¦ | é£é™© |
|------|---------|---------|------|
| æ— ç¼“å­˜ | ~10GB | 0.4 samples/s | I/Oç“¶é¢ˆ |
| å…¨é‡ç¼“å­˜ | ~290GB Ã— workers | ç†è®ºæœ€å¿« | OOMå´©æºƒ |
| **LRUç¼“å­˜(50GB)** | **~50GBç¨³å®š** | **1.5-2.5 samples/s** | **âœ… æœ€ä½³** |

---

## âš¡ æ¨¡å‹æ€§èƒ½ä¼˜åŒ–

### æ€§èƒ½ç“¶é¢ˆåˆ†æ

**åŸºäºå®æµ‹æ•°æ®ï¼ˆ111æ¬¡å‰å‘ä¼ æ’­ï¼Œå¹³å‡1789.72msï¼‰ï¼š**

1. **ğŸ”´ Stage 8 (é‡å»ºå±‚)** - å æ€»è€—æ—¶ **32.8%** (586.90ms)
2. **ğŸ”´ Stage 2 (1/2xåˆ†è¾¨ç‡)** - å æ€»è€—æ—¶ **15.2%** (271.33ms)
3. **ğŸŸ¡ Stage 1 èåˆ** - å æ€»è€—æ—¶ **8.6%** (154.02ms)

**ä¼˜åŒ–è¿™ä¸‰ä¸ªé˜¶æ®µå¯å‡å°‘çº¦50%çš„è®¡ç®—æ—¶é—´**

---

### ä¼˜åŒ–ç­–ç•¥

#### ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šæ··åˆç²¾åº¦è®­ç»ƒï¼ˆç«‹å³å®æ–½ï¼‰âœ…

**é…ç½®ï¼š**
```yaml
TRAIN:
  AMP_ENABLED: true        # è‡ªåŠ¨æ··åˆç²¾åº¦
  AMP_OPT_LEVEL: "O1"      # O1: ä¿æŒå¤§éƒ¨åˆ†æ“ä½œåœ¨FP16
```

**ä»£ç ä¿®æ”¹ (src/train.py)ï¼š**
```python
from torch.cuda.amp import autocast, GradScaler

def train(...):
    # åˆå§‹åŒ– GradScaler
    amp_enabled = cfg.get("TRAIN", {}).get("AMP_ENABLED", False)
    scaler = GradScaler() if amp_enabled else None
    
    for step, batch in enumerate(train_loader):
        optimizer.zero_grad()
        
        # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
        with autocast(enabled=amp_enabled):
            out = model(rgb, spike)
            loss_dict = criterion(out, y_gt)
            loss = sum(loss_dict.values())
        
        # åå‘ä¼ æ’­
        if scaler is not None:
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            optimizer.step()
```

**é¢„æœŸæ•ˆæœï¼š** å‡å°‘ 30-40% è®¡ç®—æ—¶é—´ï¼Œé™ä½æ˜¾å­˜ä½¿ç”¨

---

#### ç¬¬äºŒä¼˜å…ˆçº§ï¼šä¼˜åŒ–Stage 8é‡å»ºå±‚

**é—®é¢˜ï¼š** Stage 8 åœ¨å…¨åˆ†è¾¨ç‡ä¸Šæ“ä½œï¼ŒåŒ…å«å¤§é‡å·ç§¯å’Œé‡å»ºæ¨¡å—

**æ–¹æ¡ˆ1ï¼šå‡å°‘Transformerå—æ·±åº¦**

```python
# ä¿®æ”¹ src/train.py ä¸­çš„ VRT åˆå§‹åŒ–ï¼š
vrt = VRT(
    upscale=1,
    in_chans=3,
    out_chans=3,
    img_size=img_size_cfg,
    window_size=window_size_cfg,
    embed_dims=embed_dims_cfg,
    depths=[8, 8, 8, 8, 4, 4, 4, 4],  # Stage 8 ä»8é™åˆ°4
    #      â†‘ Stage 1-7 ä¿æŒä¸å˜   â†‘ å‡åŠ
    use_checkpoint_attn=True,
    use_checkpoint_ffn=True,
)
```

**é¢„æœŸæ•ˆæœï¼š** å‡å°‘ 15-25% Stage 8 è€—æ—¶

**æ–¹æ¡ˆ2ï¼šä¼˜åŒ–çª—å£å¤§å°**

```yaml
MODEL:
  WINDOW_SIZE: 6  # ä» 8 é™åˆ° 6ï¼ˆå‡å°‘çº¦ 44% çš„æ³¨æ„åŠ›è®¡ç®—é‡ï¼‰
```

**é¢„æœŸæ•ˆæœï¼š** å‡å°‘ 20-30% Stage 8 è€—æ—¶ï¼Œç²¾åº¦æŸå¤±é€šå¸¸<0.5dB

---

#### ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šä¼˜åŒ–Stage 2

**é—®é¢˜ï¼š** Stage 2 åœ¨ 1/2x åˆ†è¾¨ç‡ (128Ã—128) ä¸Šæ“ä½œï¼ŒTransformerå—è¾ƒå¤š

**æ–¹æ¡ˆï¼šå‡å°‘Stage 2çš„å—æ•°é‡**

```python
vrt = VRT(
    depths=[8, 6, 8, 8, 4, 4, 4, 4],  # Stage 2 ä»8é™åˆ°6
    #         â†‘ å‡å°‘2ä¸ªå—
    # ...
)
```

**é¢„æœŸæ•ˆæœï¼š** å‡å°‘ 25% Stage 2 è€—æ—¶

---

#### ç¬¬å››ä¼˜å…ˆçº§ï¼šä¼˜åŒ–èåˆæ¨¡å—

**æ–¹æ¡ˆï¼šå¢å¤§èåˆchunk_size**

```yaml
MODEL:
  FUSE:
    HEADS: 4
    ADAPTIVE_CHUNK: true
    MAX_BATCH_TOKENS: 65536  # ä» 49152 å¢åŠ 
    CHUNK_SIZE: 96           # ä» 64 å¢åŠ 
    CHUNK_SHAPE: "square"
```

**é¢„æœŸæ•ˆæœï¼š** å‡å°‘ 20-30% èåˆè€—æ—¶

---

### é«˜çº§ä¼˜åŒ–æŠ€æœ¯

#### 1. torch.compile() (PyTorch 2.0+)

```python
# åœ¨æ¨¡å‹åˆ›å»ºåæ·»åŠ ï¼š
if torch.__version__ >= "2.0.0" and cfg.get("TRAIN", {}).get("COMPILE_MODEL", False):
    model = torch.compile(model, mode="max-autotune")
```

#### 2. æ¸è¿›å¼è®­ç»ƒç­–ç•¥

```yaml
# å…ˆç”¨å°åˆ†è¾¨ç‡è®­ç»ƒï¼Œå†å¾®è°ƒå¤§åˆ†è¾¨ç‡
DATA:
  TRAIN_CROP_SIZE: 128  # å‰40 epochs
  # TRAIN_CROP_SIZE: 256  # å40 epochs
```

#### 3. çŸ¥è¯†è’¸é¦

```python
# åˆ›å»ºè½»é‡çº§å­¦ç”Ÿæ¨¡å‹
vrt_student = VRT(
    depths=[4, 4, 4, 4, 2, 2, 2, 2],  # æ·±åº¦å‡åŠ
    # ...
)
```

---

## ğŸ”„ DataLoaderä¼˜åŒ–

### æ ¸å¿ƒé…ç½®

```yaml
DATA:
  USE_PRECOMPUTED_VOXELS: false  # å®æ—¶æ¨¡å¼ï¼ŒèŠ‚çœç£ç›˜
  USE_RAM_CACHE: true
  CACHE_SIZE_GB: 4.0

DATALOADER:
  TOTAL_WORKERS: "auto"          # æˆ–æŒ‡å®šæ•°å­—ï¼Œå¦‚ 32
  TRAIN_PREFETCH_FACTOR: 4       # æ¯workeré¢„å–4ä¸ªbatch
  VAL_PREFETCH_FACTOR: 2
  PIN_MEMORY: true               # ä½¿ç”¨é”é¡µå†…å­˜
  PERSISTENT_WORKERS: true       # ä¿æŒworkerè¿›ç¨‹
  DROP_LAST: true
```

### Workeræ•°é‡è‡ªåŠ¨è®¡ç®—

```python
# TOTAL_WORKERS: "auto" çš„è®¡ç®—é€»è¾‘
if TOTAL_WORKERS == "auto":
    total_workers = int(cpu_count() * 0.8)  # ä½¿ç”¨80%çš„CPUæ ¸å¿ƒ
    workers_per_gpu = total_workers // num_gpus
    workers_per_gpu = max(4, workers_per_gpu)  # è‡³å°‘4ä¸ª
```

**ç¤ºä¾‹ï¼š**
- 40æ ¸CPU, 3 GPUs: `32 workers` â†’ æ¯GPU 10 workers
- 64æ ¸CPU, 4 GPUs: `51 workers` â†’ æ¯GPU 12 workers

### æ€§èƒ½è°ƒä¼˜å»ºè®®

#### CPUå¯†é›†å‹ç³»ç»Ÿ
```yaml
DATALOADER:
  TOTAL_WORKERS: 32           # cpu_count * 0.8
  TRAIN_PREFETCH_FACTOR: 4
```

#### å†…å­˜å—é™ç³»ç»Ÿ
```yaml
DATALOADER:
  TOTAL_WORKERS: 16           # cpu_count * 0.5
  TRAIN_PREFETCH_FACTOR: 2
DATA:
  CACHE_SIZE_GB: 2.0          # å‡å°ç¼“å­˜
```

---

## ğŸ¯ ç»¼åˆä¼˜åŒ–æ–¹æ¡ˆ

### æ–¹æ¡ˆ Aï¼šä¿å®ˆä¼˜åŒ–ï¼ˆç²¾åº¦ä¼˜å…ˆï¼‰

**æ¨èåœºæ™¯ï¼š** é¦–æ¬¡ä¼˜åŒ–ï¼Œé‡è§†ç²¾åº¦ç¨³å®šæ€§

**é…ç½®æ–‡ä»¶ (configs/deblur/vrt_spike_opt.yaml)ï¼š**
```yaml
MODEL:
  VRT_DEPTHS: [8, 7, 8, 8, 4, 4, 4, 6]  # Stage 2: 8â†’7, Stage 8: 8â†’6
  WINDOW_SIZE: 7                         # ä» 8 é™åˆ° 7
  FUSE:
    HEADS: 4
    ADAPTIVE_CHUNK: true
    MAX_BATCH_TOKENS: 65536
    CHUNK_SIZE: 80                       # ä» 64 å¢åŠ 
    CHUNK_SHAPE: "square"

DATA:
  USE_RAM_CACHE: true
  CACHE_SIZE_GB: 50.0

DATALOADER:
  TOTAL_WORKERS: "auto"
  TRAIN_PREFETCH_FACTOR: 4
  PIN_MEMORY: true
  PERSISTENT_WORKERS: true

TRAIN:
  AMP_ENABLED: true                      # å¯ç”¨æ··åˆç²¾åº¦
  AMP_OPT_LEVEL: "O1"
  BATCH_SIZE: 1
```

**é¢„æœŸæå‡ï¼š** 
- æ€§èƒ½: +25-35%
- è€—æ—¶: 1789ms â†’ 1200-1300ms
- ç²¾åº¦å½±å“: <0.3dB

---

### æ–¹æ¡ˆ Bï¼šæ¿€è¿›ä¼˜åŒ–ï¼ˆæ€§èƒ½ä¼˜å…ˆï¼‰

**æ¨èåœºæ™¯ï¼š** éœ€è¦å¿«é€Ÿè¿­ä»£ï¼Œå¯æ¥å—é€‚åº¦ç²¾åº¦æŸå¤±

**é…ç½®æ–‡ä»¶ (configs/deblur/vrt_spike_fast.yaml)ï¼š**
```yaml
MODEL:
  VRT_DEPTHS: [8, 6, 6, 8, 4, 4, 4, 4]  # Stage 2: 8â†’6, Stage 8: 8â†’4
  WINDOW_SIZE: 6                         # ä» 8 é™åˆ° 6
  FUSE:
    HEADS: 4
    ADAPTIVE_CHUNK: true
    MAX_BATCH_TOKENS: 65536
    CHUNK_SIZE: 96
    CHUNK_SHAPE: "square"
  SPIKE_TSA:
    HEADS: 4
    ADAPTIVE_CHUNK: true
    MAX_BATCH_TOKENS: 65536
    CHUNK_SIZE: 96
    CHUNK_SHAPE: "square"

DATA:
  USE_RAM_CACHE: true
  CACHE_SIZE_GB: 50.0
  TRAIN_CROP_SIZE: 128                   # å…ˆç”¨å°å°ºå¯¸è®­ç»ƒ

DATALOADER:
  TOTAL_WORKERS: "auto"
  TRAIN_PREFETCH_FACTOR: 6               # å¢å¤§é¢„å–
  PIN_MEMORY: true
  PERSISTENT_WORKERS: true

TRAIN:
  AMP_ENABLED: true
  AMP_OPT_LEVEL: "O1"
  BATCH_SIZE: 2                          # æ··åˆç²¾åº¦ä¸‹å¯å¢å¤§
```

**é¢„æœŸæå‡ï¼š**
- æ€§èƒ½: +45-60%
- è€—æ—¶: 1789ms â†’ 800-1000ms
- ç²¾åº¦å½±å“: 0.5-1.0dB

---

### æ–¹æ¡ˆ Cï¼šå†…å­˜å—é™ä¼˜åŒ–

**æ¨èåœºæ™¯ï¼š** 64-128GBå†…å­˜ç³»ç»Ÿï¼Œæˆ–å…±äº«æœåŠ¡å™¨

**é…ç½®æ–‡ä»¶ (configs/deblur/vrt_spike_lowmem.yaml)ï¼š**
```yaml
MODEL:
  VRT_DEPTHS: [8, 7, 8, 8, 4, 4, 4, 6]
  WINDOW_SIZE: 7

DATA:
  USE_RAM_CACHE: true
  CACHE_SIZE_GB: 20.0                    # å‡å°ç¼“å­˜

DATALOADER:
  TOTAL_WORKERS: 8                       # å‡å°‘workers
  TRAIN_PREFETCH_FACTOR: 2               # å‡å°é¢„å–
  PIN_MEMORY: true
  PERSISTENT_WORKERS: true

TRAIN:
  AMP_ENABLED: true
  AMP_OPT_LEVEL: "O1"
  BATCH_SIZE: 1
  GRADIENT_ACCUMULATION_STEPS: 4         # æ¢¯åº¦ç´¯ç§¯è¡¥å¿å°batch
```

---

### å®æ–½æ­¥éª¤

#### Step 1: åˆ›å»ºä¼˜åŒ–é…ç½®æ–‡ä»¶

```bash
# åˆ›å»ºä¿å®ˆä¼˜åŒ–ç‰ˆæœ¬
cp configs/deblur/vrt_spike_baseline.yaml \
   configs/deblur/vrt_spike_opt.yaml

# åˆ›å»ºæ¿€è¿›ä¼˜åŒ–ç‰ˆæœ¬
cp configs/deblur/vrt_spike_baseline.yaml \
   configs/deblur/vrt_spike_fast.yaml
```

#### Step 2: ä¿®æ”¹ VRT åˆå§‹åŒ–ä»£ç 

åœ¨ `src/train.py` çš„ `create_model()` å‡½æ•°ä¸­ï¼š

```python
def create_model(cfg: dict, device: torch.device) -> nn.Module:
    # ä»é…ç½®è¯»å– VRT depthsï¼ˆå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
    vrt_depths = cfg.get("MODEL", {}).get("VRT_DEPTHS", [8, 8, 8, 8, 4, 4, 4, 8])
    
    vrt = VRT(
        upscale=1,
        in_chans=3,
        out_chans=3,
        img_size=img_size_cfg,
        window_size=window_size_cfg,
        embed_dims=embed_dims_cfg,
        depths=vrt_depths,  # â† ä½¿ç”¨é…ç½®ä¸­çš„depths
        use_checkpoint_attn=True,
        use_checkpoint_ffn=True,
    )
    # ... å…¶ä½™ä»£ç  ...
```

#### Step 3: è¿è¡Œä¼˜åŒ–é…ç½®

```bash
# æµ‹è¯•ä¿å®ˆä¼˜åŒ–ç‰ˆæœ¬
python src/train.py --config configs/deblur/vrt_spike_opt.yaml

# æµ‹è¯•æ¿€è¿›ä¼˜åŒ–ç‰ˆæœ¬
python src/train.py --config configs/deblur/vrt_spike_fast.yaml

# è¿è¡Œæ€§èƒ½åˆ†æ
python analyze_performance.py outputs/logs/train_<timestamp>.log
```

---

## ğŸ“Š ç›‘æ§å’Œè°ƒè¯•

### 1. å†…å­˜ç›‘æ§

#### è‡ªåŠ¨ç›‘æ§

è®­ç»ƒæ—¶è‡ªåŠ¨è¾“å‡ºå†…å­˜ç»Ÿè®¡ï¼ˆæ¯100æ­¥ï¼‰ï¼š

```
[After dataset creation] Memory - Process: 12345.6MB (4.8%), System: 48.3/256.0GB (18.9%)
[Step 100] Memory - Process: 15678.9MB (6.1%), System: 125.4/256.0GB (49.0%)
âš ï¸  WARNING: System memory usage is critically high (91.2%)!
   Consider reducing CACHE_SIZE_GB or TOTAL_WORKERS in config.
```

#### ç¼“å­˜ç»Ÿè®¡

```python
# åœ¨æ•°æ®é›†å¯¹è±¡ä¸Šè°ƒç”¨
train_set.print_cache_stats()
# è¾“å‡ºï¼š
# [SpikeDeblurDataset] Cache Stats:
#   - Size: 374 items
#   - Memory: 49876.3 / 51200.0 MB
#   - Hit Rate: 82.45% (8245 hits / 1755 misses)
```

#### ç³»ç»Ÿçº§ç›‘æ§

```bash
# å®æ—¶æŸ¥çœ‹å†…å­˜å’ŒGPU
watch -n 1 'free -h && nvidia-smi'

# æŸ¥çœ‹è¿›ç¨‹å†…å­˜æ’åº
ps aux --sort=-rss | head -20

# ç£ç›˜I/Oç›‘æ§
iostat -x 1
```

### 2. æ€§èƒ½ç›‘æ§

#### æ•°æ®åŠ è½½æ—¶é—´

```python
# æ·»åŠ åˆ° train.py
import time

data_time = 0
start = time.time()

for batch in train_loader:
    data_time += time.time() - start
    
    # ... training code ...
    
    if step % 50 == 0:
        print(f"Avg data time: {data_time/50*1000:.1f}ms")
        data_time = 0
    
    start = time.time()
```

**ç›®æ ‡ï¼š** data_time < 10ms

#### GPUåˆ©ç”¨ç‡ç›‘æ§

```bash
# æŒç»­ç›‘æ§
nvidia-smi dmon -s u

# è¯¦ç»†ç›‘æ§
watch -n 1 nvidia-smi
```

**ç›®æ ‡ï¼š** GPUåˆ©ç”¨ç‡ > 90%

---

## ğŸ” æ•…éšœæ’é™¤

### é—®é¢˜1ï¼šå†…å­˜ä¸è¶³ (OOM)

**ç—‡çŠ¶ï¼š**
```
RuntimeError: Out of memory
æˆ–
ç³»ç»Ÿå†…å­˜ä½¿ç”¨100%ï¼Œè¿›ç¨‹è¢«kill
```

**å¯èƒ½åŸå› ï¼š**
1. CACHE_SIZE_GBè®¾ç½®è¿‡å¤§
2. TOTAL_WORKERSè¿‡å¤š
3. batch_sizeè¿‡å¤§

**è§£å†³æ–¹æ¡ˆï¼š**
```yaml
DATA:
  CACHE_SIZE_GB: 30.0  # é™ä½ç¼“å­˜å¤§å°

DATALOADER:
  TOTAL_WORKERS: 8     # å‡å°‘workers

TRAIN:
  BATCH_SIZE: 1        # é™ä½batch size
  GRADIENT_ACCUMULATION_STEPS: 4  # æ¢¯åº¦ç´¯ç§¯è¡¥å¿
```

---

### é—®é¢˜2ï¼šGPUåˆ©ç”¨ç‡ä½ (<50%)

**ç—‡çŠ¶ï¼š**
```
GPU Util: 30-50%
samples/s: <1.0
```

**å¯èƒ½åŸå› ï¼š**
1. Workeræ•°é‡ä¸è¶³
2. Prefetch factorå¤ªå°
3. ç£ç›˜I/Oç“¶é¢ˆ

**è§£å†³æ–¹æ¡ˆï¼š**
```yaml
DATALOADER:
  TOTAL_WORKERS: "auto"          # å¢åŠ workers
  TRAIN_PREFETCH_FACTOR: 4       # å¢å¤§prefetch
  PIN_MEMORY: true               # å¯ç”¨pin memory
  PERSISTENT_WORKERS: true       # ä¿æŒworkers

DATA:
  USE_RAM_CACHE: true            # å¯ç”¨ç¼“å­˜
  CACHE_SIZE_GB: 50.0
```

---

### é—®é¢˜3ï¼šè®­ç»ƒé€Ÿåº¦æ…¢ä½†GPUæ»¡è½½

**ç—‡çŠ¶ï¼š**
```
GPU Util: >90%
samples/s: <1.5
å‰å‘ä¼ æ’­æ—¶é—´: >1500ms
```

**è§£å†³æ–¹æ¡ˆï¼š** å‚è€ƒ[æ¨¡å‹æ€§èƒ½ä¼˜åŒ–](#æ¨¡å‹æ€§èƒ½ä¼˜åŒ–)

1. **å¯ç”¨æ··åˆç²¾åº¦** (æœ€ä¼˜å…ˆ)
   ```yaml
   TRAIN:
     AMP_ENABLED: true
   ```

2. **ä¼˜åŒ–æ¨¡å‹ç»“æ„**
   - å‡å°‘depths
   - é™ä½window_size
   - å¢å¤§chunk_size

---

### é—®é¢˜4ï¼šç¼“å­˜å‘½ä¸­ç‡ä½ (<60%)

**ç—‡çŠ¶ï¼š**
```
Cache hit rate: 45.2% (4520 hits / 5480 misses)
```

**åŸå› ï¼š**
- ç¼“å­˜å¤§å°ä¸è¶³
- æ•°æ®è®¿é—®æ¨¡å¼éšæœºæ€§å¼º

**è§£å†³æ–¹æ¡ˆï¼š**
```yaml
DATA:
  CACHE_SIZE_GB: 80.0  # å¢åŠ ç¼“å­˜ï¼ˆå¦‚æœ‰ä½™é‡ï¼‰

# æˆ–è°ƒæ•´é‡‡æ ·ç­–ç•¥ï¼ˆåœ¨ä»£ç ä¸­ï¼‰
# - æŒ‰åºåˆ—åˆ†ç»„é‡‡æ ·
# - å‡å°‘shuffleçš„éšæœºæ€§
```

---

### é—®é¢˜5ï¼šæ··åˆç²¾åº¦è®­ç»ƒä¸ç¨³å®š

**ç—‡çŠ¶ï¼š**
```
Lossçªç„¶å˜æˆNaN
æˆ–æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# ä½¿ç”¨æ›´ä¿å®ˆçš„scaleråˆå§‹åŒ–
scaler = GradScaler(init_scale=2.**10)  # é™ä½åˆå§‹ç¼©æ”¾

# æˆ–ä½¿ç”¨O2çº§åˆ«
TRAIN:
  AMP_OPT_LEVEL: "O2"  # æ›´æ¿€è¿›ï¼Œä½†å¯èƒ½æ›´ä¸ç¨³å®š
```

---

## ğŸ“ˆ é¢„æœŸæ€§èƒ½å¯¹æ¯”

| é…ç½® | å‰å‘ä¼ æ’­è€—æ—¶ | è®­ç»ƒé€Ÿåº¦ | ç›¸å¯¹æå‡ | ç²¾åº¦å½±å“ | å†…å­˜ä½¿ç”¨ |
|------|-------------|---------|---------|---------|---------|
| **åŸºçº¿** | 1789ms | 1.2 samples/s | - | - | ~60GB |
| **ä»…æ··åˆç²¾åº¦** | 1100-1200ms | 1.8 samples/s | +35-40% | <0.1dB | ~45GB |
| **ä¿å®ˆä¼˜åŒ–** | 1200-1300ms | 2.0 samples/s | +30-35% | <0.3dB | ~50GB |
| **æ¿€è¿›ä¼˜åŒ–** | 800-1000ms | 2.5 samples/s | +45-60% | 0.5-1.0dB | ~50GB |
| **å†…å­˜å—é™ä¼˜åŒ–** | 1300-1400ms | 1.8 samples/s | +25-30% | <0.3dB | ~30GB |

---

## âœ… éªŒè¯æ¸…å•

è®­ç»ƒå®ŒæˆåéªŒè¯ï¼š

### åŠŸèƒ½éªŒè¯
- [ ] è®­ç»ƒæŸå¤±æ›²çº¿ç¨³å®šæ”¶æ•›
- [ ] éªŒè¯é›† PSNR/SSIM åœ¨å¯æ¥å—èŒƒå›´å†…
- [ ] æ— NaNæˆ–Infå€¼å‡ºç°
- [ ] checkpointæ­£ç¡®ä¿å­˜å’ŒåŠ è½½

### æ€§èƒ½éªŒè¯
- [ ] è®­ç»ƒé€Ÿåº¦ï¼ˆsamples/sï¼‰è¾¾åˆ°ç›®æ ‡
- [ ] GPUåˆ©ç”¨ç‡ > 90%
- [ ] data_time < 10ms
- [ ] ç¼“å­˜å‘½ä¸­ç‡ > 75%

### èµ„æºéªŒè¯
- [ ] GPUæ˜¾å­˜ä½¿ç”¨é™ä½æˆ–å¯æ¥å—
- [ ] ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç¨³å®šï¼ˆæ— æ³„æ¼ï¼‰
- [ ] CPUåˆ©ç”¨ç‡ < 85%
- [ ] ç£ç›˜I/Oæ— æ˜æ˜¾ç“¶é¢ˆ

---

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

### âœ… æ¨èåšæ³•

1. **ä¼˜å…ˆå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ** - æœ€ç®€å•ã€æ”¶ç›Šæœ€å¤§
2. **ä½¿ç”¨LRUç¼“å­˜** - å¹³è¡¡å†…å­˜å’Œæ€§èƒ½
3. **è‡ªåŠ¨è®¡ç®—workeræ•°é‡** - é€‚åº”ä¸åŒç¡¬ä»¶
4. **å¯ç”¨persistent workers** - é¿å…é‡å¤åˆå§‹åŒ–
5. **å®šæœŸç›‘æ§æ€§èƒ½** - åŠæ—¶å‘ç°é—®é¢˜
6. **æ¸è¿›å¼ä¼˜åŒ–** - å…ˆä¿å®ˆå†æ¿€è¿›

### âŒ é¿å…åšæ³•

1. è®¾ç½®CACHE_SIZE_GB > ç‰©ç†å†…å­˜çš„50%
2. TOTAL_WORKERS > CPUæ ¸å¿ƒæ•°
3. åŒæ—¶è¿è¡Œå¤šä¸ªè®­ç»ƒä»»åŠ¡ï¼ˆåœ¨å†…å­˜å—é™ç³»ç»Ÿï¼‰
4. ç¦ç”¨å†…å­˜ç›‘æ§
5. è¿‡åº¦ä¼˜åŒ–å¯¼è‡´ç²¾åº¦ä¸¥é‡ä¸‹é™

---

## ğŸ“š ç›¸å…³èµ„æº

- **[é…ç½®æŒ‡å—](CONFIG_GUIDE.md)** - å®Œæ•´é…ç½®å‚æ•°è¯´æ˜
- **[æ•°æ®åŠ è½½æŒ‡å—](DATA_GUIDE.md)** - DataLoaderè¯¦ç»†é…ç½®
- **[è®­ç»ƒæŒ‡å—](QUICK_START.md)** - è®­ç»ƒæµç¨‹å’Œå‘½ä»¤
- **[æ¶æ„æ–‡æ¡£](ARCHITECTURE.md)** - æ¨¡å‹æ¶æ„å’Œæ•°æ®æµ

### å¤–éƒ¨èµ„æº

- [VRT å®˜æ–¹ä»“åº“](https://github.com/JingyunLiang/VRT)
- [PyTorch AMP æ–‡æ¡£](https://pytorch.org/docs/stable/amp.html)
- [PyTorch DataLoaderæœ€ä½³å®è·µ](https://pytorch.org/docs/stable/data.html#multi-process-data-loading)

---

## ğŸ“ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. âœ… **ç«‹å³å®æ–½ï¼š** å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆæœ€ç®€å•ï¼Œæ”¶ç›Šæœ€å¤§ï¼‰
2. â­ï¸ **çŸ­æœŸç›®æ ‡ï¼š** åˆ›å»ºå¹¶æµ‹è¯•ä¿å®ˆä¼˜åŒ–é…ç½®
3. ğŸ¯ **ä¸­æœŸç›®æ ‡ï¼š** æµ‹è¯•æ¿€è¿›ä¼˜åŒ–é…ç½®ï¼Œæ‰¾åˆ°ç²¾åº¦-æ€§èƒ½æœ€ä½³å¹³è¡¡ç‚¹
4. ğŸš€ **é•¿æœŸç›®æ ‡ï¼š** æ¢ç´¢æ¨¡å‹æ¶æ„æ”¹è¿›å’ŒçŸ¥è¯†è’¸é¦

---

**æœ€åæ›´æ–°**: 2025-10-21  
**é€‚ç”¨ç‰ˆæœ¬**: VRT+Spike v1.0+  
**ç»´æŠ¤è€…**: VRT-Spike Team


