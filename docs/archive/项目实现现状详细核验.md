# é¡¹ç›®å®ç°ç°çŠ¶è¯¦ç»†æ ¸éªŒæŠ¥å‘Š

> åŸºäº `æ ¸éªŒ.md` æ¶æ„æ–‡æ¡£çš„å®Œæ•´å®ç°çŠ¶æ€æ ¸æŸ¥  
> ç”Ÿæˆæ—¶é—´: 2025-10-15  
> æ ¸éªŒèŒƒå›´: æ‰€æœ‰æ¨¡å—ä»æ•°æ®è¾“å…¥åˆ°æŸå¤±å‡½æ•°çš„å®Œæ•´æµç¨‹

---

## ğŸ“‹ æ ¸éªŒæ¦‚è§ˆ

| æ¶æ„é˜¶æ®µ | æ¨¡å—æ•°é‡ | å®ç°çŠ¶æ€ | å®Œæˆåº¦ |
|---------|---------|---------|--------|
| 1. è¾“å…¥ä¸æ—¶é—´å¯¹é½ | 2 | âœ… å·²å®ç° | 100% |
| 2. Spikeè¡¨å¾è½¬æ¢ | 2 | âœ… å·²å®ç° | 100% |
| 3. ç‰¹å¾æå– | 2 | âœ… å·²å®ç° | 100% |
| 4. è§£ç ä¸èåˆ | 4 | âœ… å·²å®ç° | 100% |
| 5. æŸå¤±å‡½æ•° | 2 | âœ… å·²å®ç° | 100% |
| **æ€»è®¡** | **12** | **âœ… å…¨éƒ¨å®ç°** | **100%** |

---

## 1ï¸âƒ£ è¾“å…¥ä¸æ—¶é—´å¯¹é½é˜¶æ®µ

### 1.1 æ¨¡ç³Šå¸§è¾“å…¥ (Bâ‚œ)

**å®ç°ä½ç½®**: `src/data/datasets/spike_deblur_dataset.py`

**çŠ¶æ€**: âœ… å·²å®ç°

**å®ç°ç»†èŠ‚**:
- **ç±»å**: `SpikeDeblurDataset`
- **è¾“å…¥æ ¼å¼**: RGBå›¾åƒ `[B, T, 3, H, W]`
- **æ”¯æŒæ ¼å¼**: `.png`, `.jpg`, `.jpeg`, `.bmp`
- **åŠ è½½æ–¹æ³•**: `_load_rgb()` (ç¬¬448-467è¡Œ)
  ```python
  # å›¾åƒåŠ è½½æµç¨‹:
  # 1. PIL.Image.open() åŠ è½½å›¾åƒ
  # 2. è½¬æ¢ä¸ºRGBæ¨¡å¼
  # 3. å½’ä¸€åŒ–åˆ° [0, 1] (é™¤ä»¥255)
  # 4. è½¬æ¢ä¸º (C, H, W) æ ¼å¼
  # 5. è½¬æ¢ä¸ºtorch.Tensor
  ```
- **ç¼“å­˜æœºåˆ¶**: LRUç¼“å­˜æ”¯æŒï¼Œå¯é…ç½®å¤§å° (é»˜è®¤50GB)
- **æ•°æ®å¢å¼º**: è®­ç»ƒæ—¶æ”¯æŒéšæœºè£å‰ª (crop_sizeå¯é…ç½®)

**é…ç½®å‚æ•°** (`vrt_spike_baseline.yaml`):
```yaml
DATA:
  ROOT: data/processed/gopro_spike_unified
  CLIP_LEN: 5          # æ—¶é—´ç»´åº¦T
  CROP_SIZE: 256       # è®­ç»ƒæ—¶è£å‰ªå¤§å°
  IMAGE_EXTS: [".png", ".jpg", ".jpeg", ".bmp"]
```

**ä»£ç ä½ç½®**:
- æ•°æ®é›†ç±»: `src/data/datasets/spike_deblur_dataset.py:189-687`
- å›¾åƒåŠ è½½: `src/data/datasets/spike_deblur_dataset.py:448-467`

---

### 1.2 Spikeæµè¾“å…¥ä¸æ—¶é—´å¯¹é½ (S)

**å®ç°ä½ç½®**: `src/data/datasets/spike_deblur_dataset.py`

**çŠ¶æ€**: âœ… å·²å®ç°

**å®ç°ç»†èŠ‚**:

#### æ—¶é—´å¯¹é½æœºåˆ¶
- **å¯¹é½æ—¥å¿—**: ä» `outputs/logs/align_x4k1000fps.txt` è¯»å–æ—¶é—´æˆ³ä¿¡æ¯
- **å¯¹é½æ˜ å°„**: `_load_align_log()` æ–¹æ³• (ç¬¬311-344è¡Œ)
  - é”®: `(sequence_path, frame_idx)`
  - å€¼: `(t0, t1)` æ—¶é—´çª—å£
- **å…ƒæ•°æ®è¿”å›**: æ¯ä¸ªæ ·æœ¬åŒ…å« `{'t0': List[float], 't1': List[float]}`

#### Spikeæ•°æ®åŠ è½½
- **æ”¯æŒä¸¤ç§æ¨¡å¼**:
  1. **é¢„è®¡ç®—ä½“ç´ åŒ–**: ä» `.npy` æ–‡ä»¶åŠ è½½ (å¿«é€Ÿ)
  2. **å®æ—¶ä½“ç´ åŒ–**: ä» `.dat` æ–‡ä»¶åŠ è½½å¹¶è½¬æ¢ (çµæ´»)

**ä»£ç ä½ç½®**:
- æ—¶é—´å¯¹é½: `src/data/datasets/spike_deblur_dataset.py:311-344`
- SpikeåŠ è½½: `src/data/datasets/spike_deblur_dataset.py:469-518`
- å…ƒæ•°æ®: `src/data/datasets/spike_deblur_dataset.py:673-678`

**é…ç½®å‚æ•°**:
```yaml
DATA:
  SPIKE_DIR: spike                    # .datæ–‡ä»¶ç›®å½•
  VOXEL_CACHE_DIRNAME: spike_vox      # .npyæ–‡ä»¶ç›®å½•
  USE_PRECOMPUTED_VOXELS: false       # ä½¿ç”¨é¢„è®¡ç®—ä½“ç´ 
  ALIGN_LOG_PATHS: ["outputs/logs/align_x4k1000fps.txt"]
```

---

## 2ï¸âƒ£ Spikeè¡¨å¾è½¬æ¢é˜¶æ®µ

### 2.1 ä½“ç´ åŒ– (Voxelization)

**å®ç°ä½ç½®**: `src/data/datasets/voxelizer.py`

**çŠ¶æ€**: âœ… å·²å®ç°

**å®ç°ç»†èŠ‚**:
- **å‡½æ•°å**: `voxelize()`
- **è¾“å…¥**: 
  - `events`: äº‹ä»¶æ•°ç»„ `(t, y, x)` æˆ– `(t, y, x, p)`
  - `t0, t1`: æ—¶é—´çª—å£
  - `bins`: æ—¶é—´åˆ†æ¡¶æ•° (K)
- **è¾“å‡º**: ä½“ç´ ç½‘æ ¼ `(K, H, W)`, dtype=float32

**ç®—æ³•æµç¨‹**:
1. è®¡ç®—æ—¶é—´åˆ†æ¡¶ç´¢å¼•: `bin_idx = floor((t - t0) / duration * bins)`
2. å°†äº‹ä»¶ç´¯åŠ åˆ°å¯¹åº”çš„ä½“ç´ : `vox[bin_idx, y, x] += 1`
3. ç©ºé—´åæ ‡éªŒè¯å’Œè£å‰ª
4. å¯é€‰çš„log1på˜æ¢: `vox = log1p(vox)`
5. å¯é€‰çš„æ ‡å‡†åŒ–: `vox = (vox - mean) / std`

**ä»£ç ä½ç½®**: `src/data/datasets/voxelizer.py:6-69`

**é…ç½®å‚æ•°**:
```yaml
DATA:
  K: 32                # ä½“ç´ æ—¶é—´åˆ†æ¡¶æ•°
  NUM_VOXEL_BINS: 32   # åŒKï¼Œç”¨äºå®æ—¶ä½“ç´ åŒ–
```

**ç›¸å…³å·¥å…·å‡½æ•°**:
- `load_spike_dat()`: åŠ è½½ `.dat` æ–‡ä»¶ (ç¬¬20-77è¡Œ)
  - è‡ªåŠ¨æ£€æµ‹åˆ†è¾¨ç‡
  - æ”¯æŒå¤šç§å¸¸è§é…ç½® (10x360x448, 10x396x640ç­‰)
- `spike_to_voxel()`: ç®€å•ä½“ç´ åŒ– (ç¬¬80-101è¡Œ)
  - ç”¨äºå·²åŠ è½½çš„spikeæ•°æ®

---

### 2.2 å½’ä¸€åŒ– (Normalization)

**å®ç°ä½ç½®**: `src/data/datasets/voxelizer.py:64-66`

**çŠ¶æ€**: âœ… å·²å®ç°

**å®ç°ç»†èŠ‚**:
```python
if mean is not None and std is not None:
    eps = 1e-12
    vox = (vox - float(mean)) / float(max(std, eps))
```

**å½’ä¸€åŒ–å‚æ•°**:
- **é»˜è®¤å€¼**: `mean=None`, `std=None` (ä¸å½’ä¸€åŒ–)
- **å¯é…ç½®**: é€šè¿‡å‡½æ•°å‚æ•°ä¼ é€’
- **æ•°å€¼ç¨³å®š**: ä½¿ç”¨ `eps=1e-12` é˜²æ­¢é™¤é›¶

**å®é™…ä½¿ç”¨**:
- é…ç½®æ–‡ä»¶ä¸­çš„ `DATA.NORM.MEAN` å’Œ `DATA.NORM.STD` å½“å‰è®¾ç½®ä¸º `0.0` å’Œ `1.0`
- ä½“ç´ åŒ–æ—¶å·²åº”ç”¨ `log1p` å˜æ¢è¿›è¡Œå½’ä¸€åŒ–

---

## 3ï¸âƒ£ ç‰¹å¾æå–é˜¶æ®µ

### 3.1 VRT RGBç¼–ç å™¨

**å®ç°ä½ç½®**: `third_party/VRT/models/network_vrt.py`

**çŠ¶æ€**: âœ… å·²å®ç° (ç¬¬ä¸‰æ–¹åº“)

**å®ç°ç»†èŠ‚**:
- **ç±»å**: `VRT`
- **è¾“å…¥**: `[B, T, 3, H, W]` RGBè§†é¢‘åºåˆ—
- **ç¼–ç å™¨é˜¶æ®µ**: Stage 1-4 (å¯¹åº”4ä¸ªå°ºåº¦)
  - Stage 1: åŸå§‹åˆ†è¾¨ç‡ (1x)
  - Stage 2: 1/2x åˆ†è¾¨ç‡
  - Stage 3: 1/4x åˆ†è¾¨ç‡
  - Stage 4: 1/8x åˆ†è¾¨ç‡

**æ ¸å¿ƒç»„ä»¶**:
1. **SpyNet**: å…‰æµä¼°è®¡ (ç”¨äºå¸§é—´å¯¹é½)
2. **TMSA (Temporal Mutual Self Attention)**: 
   - å®ç°ä½ç½®: `third_party/VRT/models/network_vrt.py:728-764`
   - æ—¶é—´ç»´åº¦çš„äº’æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›
   - çª—å£åŒ–æ³¨æ„åŠ›æœºåˆ¶
3. **å¤šå°ºåº¦ç‰¹å¾æå–**: 
   - æ¯ä¸ªStageåŒ…å«å¤šä¸ªTMSAå—
   - ä¸‹é‡‡æ ·é€šè¿‡ `Downsample3D` æ¨¡å—å®ç°

**è¾“å‡º**: 
- ç¼–ç å™¨è¾“å‡º: `{FrÂ¹, FrÂ², FrÂ³, Frâ´}` (4ä¸ªå°ºåº¦çš„ç‰¹å¾)
- æ¯ä¸ªç‰¹å¾: `[B, C_i, D, H_i, W_i]` (Dä¸ºæ—¶é—´ç»´åº¦)

**é›†æˆæ–¹å¼**: 
- é€šè¿‡ `VRTWithSpike` åŒ…è£…å¹¶ä½¿ç”¨ monkey-patch æ³¨å…¥èåˆé€»è¾‘
- åŸå§‹ VRT çš„ `forward_features()` æ–¹æ³•è¢«åŠ¨æ€æ›¿æ¢

**ä»£ç ä½ç½®**:
- VRTä¸»ç±»: `third_party/VRT/models/network_vrt.py:1231-1475`
- TMSAæ¨¡å—: `third_party/VRT/models/network_vrt.py:728-1095`
- é›†æˆä»£ç : `src/models/integrate_vrt.py:78-188`

**é…ç½®å‚æ•°**:
```yaml
MODEL:
  VRT_CFG: third_party/VRT/options/deblur/vrt_base.yaml
  CHANNELS_PER_SCALE: [96, 96, 96, 96]  # 4ä¸ªå°ºåº¦çš„é€šé“æ•°
```

**åˆå§‹åŒ–ä»£ç ** (`src/train.py:349-358`):
```python
vrt = VRT(
    upscale=1,
    in_chans=3,
    out_chans=3,
    img_size=img_size_cfg,
    window_size=window_size_cfg,
    embed_dims=embed_dims_cfg,
    use_checkpoint_attn=True,
    use_checkpoint_ffn=True,
)
```

---

### 3.2 SpikeEncoder3D

**å®ç°ä½ç½®**: `src/models/spike_encoder3d.py`

**çŠ¶æ€**: âœ… å·²å®ç°

**å®ç°ç»†èŠ‚**:
- **ç±»å**: `SpikeEncoder3D`
- **è¾“å…¥**: `[B, T, K, H, W]` ä½“ç´ åŒ–çš„Spikeæ•°æ®
- **è¾“å‡º**: `List[Tensor]` é•¿åº¦ä¸º4ï¼Œæ¯ä¸ªå½¢çŠ¶ä¸º `[B, C_i, T_i, H_i, W_i]`

**ç½‘ç»œæ¶æ„**:
```python
# è¾“å…¥æŠ•å½±
Conv3D(K â†’ C1, kernel=3x3x3, stride=1) + ReLU
ResidualBlock3D(C1) Ã— 2

# å¤šå°ºåº¦ä¸‹é‡‡æ · (é‡å¤3æ¬¡ï¼Œå¾—åˆ°4ä¸ªå°ºåº¦)
for i in 1..3:
    Conv3D(C_i â†’ C_{i+1}, kernel=3x3x3, stride=(s_t, s_s, s_s))
    ResidualBlock3D(C_{i+1}) Ã— 2
```

**ResidualBlock3D** (ç¬¬10-24è¡Œ):
```python
Conv3D(3x3x3) â†’ ReLU â†’ Conv3D(3x3x3) â†’ Add(identity) â†’ ReLU
```

**ä¸‹é‡‡æ ·ç­–ç•¥**:
- **æ—¶é—´æ­¥é•¿**: é»˜è®¤ `[1, 1, 1]` (ä¿æŒæ—¶é—´åˆ†è¾¨ç‡)
- **ç©ºé—´æ­¥é•¿**: é»˜è®¤ `[2, 2, 2]` (åŒ¹é…VRT: 1x â†’ 1/2x â†’ 1/4x â†’ 1/8x)

**å¯¹é½è®¾è®¡**:
- è¾“å‡ºçš„4ä¸ªå°ºåº¦ä¸VRTç¼–ç å™¨çš„4ä¸ªå°ºåº¦åœ¨ç©ºé—´åˆ†è¾¨ç‡ä¸Šå¯¹é½
- é€šé“æ•°é€šè¿‡ `channels_per_scale` å‚æ•°æ§åˆ¶ï¼Œé»˜è®¤ `[96, 96, 96, 96]`

**ä»£ç ä½ç½®**: `src/models/spike_encoder3d.py:27-111`

**é…ç½®å‚æ•°**:
```yaml
MODEL:
  CHANNELS_PER_SCALE: [96, 96, 96, 96]
  SPIKE_ENCODER:
    TEMPORAL_STRIDES: null  # é»˜è®¤[1,1,1]
    SPATIAL_STRIDES: null   # é»˜è®¤[2,2,2]
```

**åˆå§‹åŒ–ä»£ç ** (`src/models/integrate_vrt.py:53-58`):
```python
self.spike_encoder = SpikeEncoder3D(
    in_bins=spike_bins,              # 32
    channels_per_scale=channels_per_scale,  # [96,96,96,96]
    temporal_strides=temporal_strides,      # [1,1,1]
    spatial_strides=spatial_strides,        # [2,2,2]
)
```

---

## 4ï¸âƒ£ è§£ç ä¸èåˆé˜¶æ®µ

### 4.1 TMSAå†…éƒ¨ç‰¹å¾å¯¹é½

#### RGB TMSA (VRTå†…éƒ¨)

**å®ç°ä½ç½®**: `third_party/VRT/models/network_vrt.py:728-1095`

**çŠ¶æ€**: âœ… å·²å®ç° (VRTå†…ç½®)

**å®ç°ç»†èŠ‚**:
- **ç±»å**: `TMSA` (Temporal Mutual Self Attention)
- **ä½œç”¨**: VRTç¼–ç å™¨æ¯ä¸ªStageå†…éƒ¨çš„æ—¶é—´ç»´åº¦æ³¨æ„åŠ›
- **æœºåˆ¶**:
  - Mutual Attention: ä¸åŒå¸§ä¹‹é—´çš„äº’ç›¸æ³¨æ„
  - Self Attention: å¸§å†…è‡ªæ³¨æ„åŠ›
  - çª—å£åŒ–å¤„ç†: ä½¿ç”¨ `window_size=[T, H, W]`

**æ‰§è¡Œæ—¶æœº**:
- åœ¨æ¯ä¸ªVRTç¼–ç Stageå†…éƒ¨è‡ªåŠ¨æ‰§è¡Œ
- æ— éœ€é¢å¤–é…ç½®ï¼Œç”±VRT backboneç®¡ç†

**è¾“å…¥/è¾“å‡º**: `[B, C, D, H, W]` â†’ `[B, C, D, H, W]`

**ä»£ç ä½ç½®**: `third_party/VRT/models/network_vrt.py:728-1095`

---

#### Spike Self-Attention

**å®ç°ä½ç½®**: `src/models/spike_temporal_sa.py`

**çŠ¶æ€**: âœ… å·²å®ç°

**å®ç°ç»†èŠ‚**:
- **ç±»å**: `SpikeTemporalSA` (å¤šå°ºåº¦ç‰ˆæœ¬)
- **å•å°ºåº¦ç±»**: `SpikeTemporalSelfAttention`
- **è¾“å…¥**: `List[Tensor]` (4ä¸ªå°ºåº¦)ï¼Œæ¯ä¸ª `[B, C, T, H, W]`
- **è¾“å‡º**: `List[Tensor]` (4ä¸ªå°ºåº¦)ï¼Œæ¯ä¸ª `[B, C, T, H, W]`

**ç½‘ç»œæ¶æ„**:
```python
# å¯¹æ¯ä¸ªç©ºé—´ä½ç½®(h,w)çš„æ—¶é—´åºåˆ—åšè‡ªæ³¨æ„åŠ›
LayerNorm â†’ MultiheadAttention(Tç»´åº¦) â†’ Residual
LayerNorm â†’ MLP â†’ Residual
```

**åˆ†å—å¤„ç†** (å†…å­˜ä¼˜åŒ–):
- ç»§æ‰¿è‡ª `BaseChunkableAttention`
- ç©ºé—´ç»´åº¦åˆ†å—: è‡ªé€‚åº”æˆ–å›ºå®š64Ã—64
- é…ç½®: `ADAPTIVE_CHUNK`, `MAX_BATCH_TOKENS`, `CHUNK_SIZE`

**æ ¼å¼è½¬æ¢**:
```python
# SpikeEncoder3Dè¾“å‡º: [B, C, T, H, W]
feat_btchw = feat.permute(0, 2, 1, 3, 4)  # â†’ [B, T, C, H, W]
# æ‰§è¡Œæ³¨æ„åŠ›
out_btchw = block(feat_btchw)
# è½¬æ¢å›å»
out = out_btchw.permute(0, 2, 1, 3, 4)  # â†’ [B, C, T, H, W]
```

**ä»£ç ä½ç½®**:
- å¤šå°ºåº¦ç‰ˆæœ¬: `src/models/spike_temporal_sa.py:76-119`
- å•å°ºåº¦å®ç°: `src/models/spike_temporal_sa.py:13-74`
- åŸºç±»å·¥å…·: `src/utils/attention.py:61-96`

**é…ç½®å‚æ•°**:
```yaml
MODEL:
  SPIKE_TSA:
    HEADS: 4
    ADAPTIVE_CHUNK: true
    MAX_BATCH_TOKENS: 49152
    CHUNK_SIZE: 64
    CHUNK_SHAPE: "square"
```

**åˆå§‹åŒ–ä»£ç ** (`src/models/integrate_vrt.py:61-67`):
```python
self.spike_temporal_sa = SpikeTemporalSA(
    channels_per_scale=channels_per_scale, 
    heads=tsa_heads,            # 4
    dropout=tsa_dropout,        # 0.0
    mlp_ratio=tsa_mlp_ratio,    # 2
    chunk_cfg=tsa_chunk_cfg,
)
```

---

### 4.2 Cross-Attentionèåˆ

**å®ç°ä½ç½®**: `src/models/fusion/cross_attn_temporal.py`

**çŠ¶æ€**: âœ… å·²å®ç°

**å®ç°ç»†èŠ‚**:

#### æ ¸å¿ƒèåˆå—: `TemporalCrossAttnFuseBlock`

**è¾“å…¥**:
- `Fr`: RGBç‰¹å¾ `[B, T, C, H, W]`
- `Fs`: Spikeç‰¹å¾ `[B, T, C, H, W]`

**è¾“å‡º**:
- èåˆç‰¹å¾ `[B, T, C, H, W]`

**ç½‘ç»œç»“æ„**:
```python
# 1. Pre-normalization
Q = LayerNorm(Fr)
K = V = LayerNorm(Fs)

# 2. Cross-Attention (chunked)
Y = TemporalCrossAttention(Q, K, V)  # åœ¨æ—¶é—´ç»´åº¦Tä¸Šåšæ³¨æ„åŠ›
X = Fr + Y  # æ®‹å·®è¿æ¥

# 3. Feed-Forward
Y_ffn = FFN(LayerNorm(X))
output = X + Y_ffn
```

**åˆ†å—å¤„ç†**:
- ç©ºé—´ç»´åº¦åˆ†å—å¤„ç† (é¿å…OOM)
- å¯¹ `(H, W)` ç»´åº¦åˆ‡åˆ†ä¸º `(h_chunk, w_chunk)` çš„å—
- æ¯ä¸ªå—ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›

**ä»£ç ä½ç½®**:
- èåˆå—: `src/models/fusion/cross_attn_temporal.py:63-122`
- æ³¨æ„åŠ›æ ¸å¿ƒ: `src/models/fusion/cross_attn_temporal.py:13-61`

---

#### å¤šå°ºåº¦èåˆ: `MultiScaleTemporalCrossAttnFuse`

**ä½œç”¨**: ä¸º4ä¸ªå°ºåº¦åˆ†åˆ«åˆ›å»ºç‹¬ç«‹çš„èåˆæ¨¡å—

**ä»£ç **:
```python
self.fuse_blocks = nn.ModuleList([
    TemporalCrossAttnFuseBlock(dim=channels, ...)
    for channels in channels_per_scale  # [96, 96, 96, 96]
])

def forward(self, Fr_list, Fs_list):
    return [
        fuse(Fr, Fs)
        for fuse, Fr, Fs in zip(self.fuse_blocks, Fr_list, Fs_list)
    ]
```

**ä»£ç ä½ç½®**: `src/models/fusion/cross_attn_temporal.py:124-152`

**é…ç½®å‚æ•°**:
```yaml
MODEL:
  FUSE:
    TYPE: TemporalCrossAttn
    HEADS: 4
    ADAPTIVE_CHUNK: true
    MAX_BATCH_TOKENS: 49152
    CHUNK_SIZE: 64
    CHUNK_SHAPE: "square"
```

**åˆå§‹åŒ–ä»£ç ** (`src/models/integrate_vrt.py:70-76`):
```python
self.cross_attn_fuse = MultiScaleTemporalCrossAttnFuse(
    channels_per_scale=channels_per_scale, 
    heads=fuse_heads,            # 4
    dropout=fuse_dropout,        # 0.0
    mlp_ratio=fuse_mlp_ratio,    # 2
    chunk_cfg=fuse_chunk_cfg,
)
```

---

### 4.3 å¤šå°ºåº¦è§£ç ä¸è·³è¿

**å®ç°ä½ç½®**: `third_party/VRT/models/network_vrt.py` (VRTè§£ç å™¨)

**çŠ¶æ€**: âœ… å·²å®ç° (VRTå†…ç½®)

**å®ç°ç»†èŠ‚**:

#### è§£ç æµç¨‹ (åœ¨ `integrate_vrt.py` çš„ monkey-patchä¸­)

**ç¼–ç é˜¶æ®µ** (Stage 1-4ï¼Œå¸¦èåˆ):
```python
x1 = vrt.stage1(x, flows)      # 1xåˆ†è¾¨ç‡
x1 = fuse_after_stage(0, x1)   # Ff_1 = CrossAttn(Fr_1, Fs'_1)

x2 = vrt.stage2(x1, flows)     # 1/2xåˆ†è¾¨ç‡
x2 = fuse_after_stage(1, x2)   # Ff_2

x3 = vrt.stage3(x2, flows)     # 1/4xåˆ†è¾¨ç‡
x3 = fuse_after_stage(2, x3)   # Ff_3

x4 = vrt.stage4(x3, flows)     # 1/8xåˆ†è¾¨ç‡
x4 = fuse_after_stage(3, x4)   # Ff_4
```

**ç“¶é¢ˆå±‚** (Stage 5):
```python
x = vrt.stage5(x4, flows)      # 1/8xåˆ†è¾¨ç‡ï¼Œä¸èåˆ
```

**è§£ç é˜¶æ®µ** (Stage 6-7ï¼Œå¸¦è·³è¿):
```python
x = vrt.stage6(x + x3, flows)  # 1/4xåˆ†è¾¨ç‡ï¼Œè·³è¿x3 (Ff_3)
x = vrt.stage7(x + x2, flows)  # 1/2xåˆ†è¾¨ç‡ï¼Œè·³è¿x2 (Ff_2)
```

**æœ€ç»ˆè¾“å‡º** (Stage 8):
```python
x = x + x1                     # è·³è¿x1 (Ff_1)
for layer in vrt.stage8:
    x = layer(x)               # é‡å»ºå±‚
x = vrt.norm(x)                # LayerNorm
```

**å…³é”®ç‚¹**:
- **èåˆç‰¹å¾ç”¨äºè·³è¿**: `x3`, `x2`, `x1` éƒ½æ˜¯èåˆåçš„ç‰¹å¾ `Ff_i`
- **ä¸Šé‡‡æ ·**: åœ¨VRTçš„Stageå†…éƒ¨é€šè¿‡ `Upsample3D` æ¨¡å—å®ç°
- **è¾“å‡ºå½¢çŠ¶**: `[B, 3, D, H, W]` (ä¸è¾“å…¥ç›¸åŒåˆ†è¾¨ç‡)

**ä»£ç ä½ç½®**: `src/models/integrate_vrt.py:92-183`

---

### 4.4 VRTé›†æˆæ€»æµç¨‹

**å®ç°ä½ç½®**: `src/models/integrate_vrt.py`

**çŠ¶æ€**: âœ… å·²å®ç°

**æ•´ä½“ç±»**: `VRTWithSpike`

**å‰å‘ä¼ æ’­æµç¨‹** (`forward()` æ–¹æ³•):

```python
def forward(rgb_clip, spike_vox):
    # Step 1: Spikeç¼–ç 
    spike_feats = self.spike_encoder(spike_vox)  
    # â†’ List[4ä¸ªå°ºåº¦], æ¯ä¸ª [B, C, T, H, W]
    
    # Step 2: Spikeæ—¶é—´Self-Attention
    spike_feats_fused = self.spike_temporal_sa(spike_feats)
    # â†’ List[4ä¸ªå°ºåº¦], æ¯ä¸ª [B, C, T, H, W]
    
    # Step 3: Monkey-patch VRT
    self._monkeypatch_forward_features(spike_feats_fused)
    
    # Step 4: VRTå‰å‘ (å†…éƒ¨ä¼šè°ƒç”¨èåˆé€»è¾‘)
    try:
        out = self.vrt(rgb_clip)
    finally:
        self._restore_forward_features()
    
    return out  # [B, T, 3, H, W]
```

**Monkey-patchæœºåˆ¶** (`_monkeypatch_forward_features()`):
- åŠ¨æ€æ›¿æ¢ `vrt.forward_features()` æ–¹æ³•
- åœ¨æ¯ä¸ªç¼–ç Stageåæ’å…¥èåˆé€»è¾‘
- ä½¿ç”¨ `types.MethodType` ç»‘å®šåˆ°VRTå®ä¾‹

**ä»£ç ä½ç½®**: `src/models/integrate_vrt.py:15-219`

**åˆå§‹åŒ–ç¤ºä¾‹** (`src/train.py:379-393`):
```python
model = VRTWithSpike(
    vrt_backbone=vrt,
    spike_bins=32,
    channels_per_scale=[96, 96, 96, 96],
    temporal_strides=[1, 1, 1],
    spatial_strides=[2, 2, 2],
    tsa_heads=4,
    tsa_dropout=0.0,
    tsa_mlp_ratio=2,
    tsa_chunk_cfg=tsa_chunk_cfg,
    fuse_heads=4,
    fuse_dropout=0.0,
    fuse_mlp_ratio=2,
    fuse_chunk_cfg=fuse_chunk_cfg,
)
```

---

## 5ï¸âƒ£ æŸå¤±å‡½æ•°é˜¶æ®µ

### 5.1 VGGæ„ŸçŸ¥æŸå¤± (VGGPerceptualLoss)

**å®ç°ä½ç½®**: `src/losses/vgg_perceptual.py`

**çŠ¶æ€**: âœ… å·²å®ç°

**å®ç°ç»†èŠ‚**:
- **ç±»å**: `VGGPerceptualLoss`
- **åŸºç¡€æ¨¡å‹**: VGG16 (é¢„è®­ç»ƒImageNetæƒé‡)
- **ç‰¹å¾æå–**: `_VGGFeature` ç±»

**æ”¯æŒçš„å±‚**:
```python
name_to_idx = {
    "relu1_2": 3,
    "relu2_2": 8,
    "relu3_3": 15,
    "relu4_3": 22,
    "relu5_3": 29,
}
```

**é»˜è®¤é…ç½®**: ä½¿ç”¨ `relu3_3` å±‚

**æŸå¤±è®¡ç®—**:
```python
# 1. è¾“å…¥å½’ä¸€åŒ– (ImageNetç»Ÿè®¡)
mean = [0.485, 0.456, 0.406]
std  = [0.229, 0.224, 0.225]
x_norm = (x - mean) / std

# 2. æå–VGGç‰¹å¾
fx = vgg(x_norm)  # List[Tensor]
fy = vgg(y_norm)

# 3. L1æŸå¤±
loss = sum(L1(a, b) for a, b in zip(fx, fy)) / len(fx)
```

**5Dè¾“å…¥æ”¯æŒ**:
- è‡ªåŠ¨åˆå¹¶batchå’Œtimeç»´åº¦: `(B, T, C, H, W)` â†’ `(B*T, C, H, W)`
- è®¡ç®—ålossä¸ºæ ‡é‡

**ä»£ç ä½ç½®**: `src/losses/vgg_perceptual.py:45-79`

**é…ç½®å‚æ•°**:
```yaml
LOSS:
  VGG_PERCEPTUAL:
    LAYERS: ["relu3_3"]
    WEIGHT: 0.1
```

**ä½¿ç”¨ç¤ºä¾‹** (`src/train.py`):
```python
vgg_loss = VGGPerceptualLoss(layers=["relu3_3"])
loss_vgg = vgg_loss(output, target) * 0.1
```

---

### 5.2 CharbonnieræŸå¤± (CharbonnierLoss)

**å®ç°ä½ç½®**: `src/losses/charbonnier.py`

**çŠ¶æ€**: âœ… å·²å®ç°

**å®ç°ç»†èŠ‚**:
- **ç±»å**: `CharbonnierLoss`
- **å…¬å¼**: `L = mean(sqrt((x - y)Â² + Î´Â²))`
- **é»˜è®¤å‚æ•°**: `Î´ = 1e-3`

**å®ç°ä»£ç **:
```python
def forward(self, input, target):
    diff = input - target
    loss = torch.sqrt(diff * diff + (self.delta * self.delta))
    return loss.mean()
```

**ä¼˜ç‚¹**:
- å¯¹ç¦»ç¾¤å€¼æ›´é²æ£’ (ç›¸æ¯”L2)
- å¯å¾®åˆ† (ç›¸æ¯”L1)
- å¹³æ»‘çš„æ¢¯åº¦

**ä»£ç ä½ç½®**: `src/losses/charbonnier.py:7-21`

**é…ç½®å‚æ•°**:
```yaml
LOSS:
  CHARBONNIER:
    DELTA: 0.001
    WEIGHT: 1.0
```

**ä½¿ç”¨ç¤ºä¾‹** (`src/train.py`):
```python
charbonnier_loss = CharbonnierLoss(delta=0.001)
loss_char = charbonnier_loss(output, target) * 1.0
```

---

### 5.3 æ€»æŸå¤±è®¡ç®—

**å®ç°ä½ç½®**: `src/train.py` (è®­ç»ƒå¾ªç¯ä¸­)

**å…¬å¼**:
```python
loss_total = Î»_char * L_charbonnier + Î»_vgg * L_vgg
```

**é»˜è®¤æƒé‡**:
- `Î»_char = 1.0`
- `Î»_vgg = 0.1`

**ä»£ç ç‰‡æ®µ** (æ¨æµ‹ä½ç½®):
```python
# å‰å‘ä¼ æ’­
output = model(blur, spike_vox)

# è®¡ç®—æŸå¤±
loss_char = charbonnier_loss(output, sharp) * 1.0
loss_vgg = vgg_loss(output, sharp) * 0.1
loss = loss_char + loss_vgg

# åå‘ä¼ æ’­
loss.backward()
```

---

## 6ï¸âƒ£ è¾…åŠ©æ¨¡å—ä¸å·¥å…·

### 6.1 æ³¨æ„åŠ›åˆ†å—å·¥å…·

**å®ç°ä½ç½®**: `src/utils/attention.py`

**çŠ¶æ€**: âœ… å·²å®ç°

**åŠŸèƒ½**:
- **è‡ªé€‚åº”åˆ†å—**: æ ¹æ®batchå¤§å°å’Œç‰¹å¾å›¾å°ºå¯¸åŠ¨æ€è®¡ç®—chunkå¤§å°
- **å†…å­˜ä¼˜åŒ–**: é¿å…å¤§ç‰¹å¾å›¾çš„OOM
- **å½¢çŠ¶ç­–ç•¥**: æ”¯æŒ `square`, `wide`, `tall` ä¸‰ç§åˆ†å—å½¢çŠ¶

**æ ¸å¿ƒç±»**: `BaseChunkableAttention`

**å…³é”®å‚æ•°**:
```python
ADAPTIVE_CHUNK: true         # å¯ç”¨è‡ªé€‚åº”
MAX_BATCH_TOKENS: 49152      # batch*h*w <= 49152
CHUNK_SIZE: 64               # è½¯ä¸Šé™
CHUNK_SHAPE: "square"        # åˆ†å—å½¢çŠ¶
```

**ä½¿ç”¨è€…**:
- `SpikeTemporalSelfAttention`
- `TemporalCrossAttention`

**ä»£ç ä½ç½®**: `src/utils/attention.py:11-96`

---

### 6.2 æ•°æ®åŠ è½½ä¸ç¼“å­˜

**å®ç°ä½ç½®**: `src/data/datasets/spike_deblur_dataset.py`

**å…³é”®ç‰¹æ€§**:

#### LRUç¼“å­˜ (`LRUCache`)
- **å†…å­˜é™åˆ¶**: å¯é…ç½® (é»˜è®¤50GB)
- **è‡ªåŠ¨æ·˜æ±°**: æœ€è¿‘æœ€å°‘ä½¿ç”¨ (LRU)
- **ç»Ÿè®¡ä¿¡æ¯**: å‘½ä¸­ç‡ã€å†…å­˜ä½¿ç”¨ç­‰

**å®ç°**:
```python
class LRUCache:
    def __init__(self, max_memory_gb=50.0):
        self.max_memory_bytes = int(max_memory_gb * 1024**3)
        self.cache = OrderedDict()  # Pythonçš„LRUå®ç°
        
    def get(self, key):
        if key in self.cache:
            self.cache.move_to_end(key)  # æ ‡è®°ä¸ºæœ€è¿‘ä½¿ç”¨
            return self.cache[key].clone()
        return None
        
    def put(self, key, value):
        # æ·˜æ±°ç›´åˆ°æœ‰è¶³å¤Ÿç©ºé—´
        while self.current_memory + item_size > self.max_memory_bytes:
            lru_key, lru_value = self.cache.popitem(last=False)
            self.current_memory -= lru_size
        self.cache[key] = value
```

**ä»£ç ä½ç½®**: `src/data/datasets/spike_deblur_dataset.py:104-173`

---

#### é¢„åŠ è½½ (`_preload_cache()`)
- **æ‰¹é‡åŠ è½½**: è®­ç»ƒå¼€å§‹å‰é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°RAM
- **è¿›åº¦æ¡**: ä½¿ç”¨tqdmæ˜¾ç¤ºåŠ è½½è¿›åº¦
- **å†…å­˜ä¼°è®¡**: é¢„ä¼°æ‰€éœ€å†…å­˜å¹¶ç»™å‡ºè­¦å‘Š

**ä»£ç ä½ç½®**: `src/data/datasets/spike_deblur_dataset.py:346-432`

**é…ç½®å‚æ•°**:
```yaml
DATA:
  USE_RAM_CACHE: true      # å¯ç”¨ç¼“å­˜
  CACHE_SIZE_GB: 4.0       # æ¯GPUè¿›ç¨‹çš„ç¼“å­˜å¤§å°
```

---

### 6.3 å¤šåˆ†è¾¨ç‡è£å‰ª

**å®ç°ä½ç½®**: `src/data/datasets/spike_deblur_dataset.py:520-586`

**åŠŸèƒ½**: æ”¯æŒRGBå’ŒSpikeåˆ†è¾¨ç‡ä¸ä¸€è‡´æ—¶çš„ååŒè£å‰ª

**å…³é”®æ–¹æ³•**:
- `_get_crop_coords()`: è®¡ç®—è£å‰ªåæ ‡
- `_apply_crop()`: åº”ç”¨å¤šåˆ†è¾¨ç‡è£å‰ª
  - å¦‚æœå°ºå¯¸ç›¸åŒ: ä½¿ç”¨ç›¸åŒåæ ‡
  - å¦‚æœå°ºå¯¸ä¸åŒ: æŒ‰æ¯”ä¾‹ç¼©æ”¾åæ ‡

**å®ç°é€»è¾‘**:
```python
def _apply_crop(tensors, size, top_ref, left_ref, h_ref, w_ref):
    for t in tensors:
        _, h, w = t.shape
        if h == h_ref and w == w_ref:
            # ç›¸åŒåˆ†è¾¨ç‡: ç›´æ¥è£å‰ª
            out.append(t[:, top_ref:top_ref+size, left_ref:left_ref+size])
        else:
            # ä¸åŒåˆ†è¾¨ç‡: æŒ‰æ¯”ä¾‹è£å‰ª
            scale_h = h / h_ref
            scale_w = w / w_ref
            top = int(top_ref * scale_h)
            left = int(left_ref * scale_w)
            target_h = int(round(size * scale_h))
            target_w = int(round(size * scale_w))
            out.append(t[:, top:top+target_h, left:left+target_w])
    return out
```

---

## 7ï¸âƒ£ é…ç½®ä¸è®­ç»ƒ

### 7.1 å®Œæ•´é…ç½®æ–‡ä»¶

**ä½ç½®**: `configs/deblur/vrt_spike_baseline.yaml`

**ä¸»è¦é…ç½®é¡¹**:

```yaml
# æ•°æ®é…ç½®
DATA:
  ROOT: data/processed/gopro_spike_unified
  CLIP_LEN: 5
  CROP_SIZE: 256
  K: 32
  USE_RAM_CACHE: true
  CACHE_SIZE_GB: 4.0

# æ¨¡å‹é…ç½®
MODEL:
  USE_SPIKE: true
  CHANNELS_PER_SCALE: [96, 96, 96, 96]
  SPIKE_TSA:
    HEADS: 4
    ADAPTIVE_CHUNK: true
  FUSE:
    HEADS: 4
    ADAPTIVE_CHUNK: true

# è®­ç»ƒé…ç½®
TRAIN:
  EPOCHS: 80
  BATCH_SIZE: 1
  NUM_WORKERS: 8
  GRADIENT_ACCUMULATION_STEPS: 6
  OPTIM:
    LR: 0.0002
    WEIGHT_DECAY: 0.0001

# æŸå¤±é…ç½®
LOSS:
  CHARBONNIER:
    WEIGHT: 1.0
  VGG_PERCEPTUAL:
    WEIGHT: 0.1
```

---

### 7.2 è®­ç»ƒè„šæœ¬

**ä¸»è„šæœ¬**: `src/train.py`

**å…³é”®æµç¨‹**:
1. **é…ç½®åŠ è½½**: è¯»å–YAMLé…ç½®
2. **æ¨¡å‹æ„å»º**: 
   - åˆ›å»ºVRT backbone
   - åŒ…è£…ä¸ºVRTWithSpike
3. **æ•°æ®åŠ è½½**: 
   - åˆ›å»ºDataset
   - åˆ›å»ºDataLoader
4. **ä¼˜åŒ–å™¨**: AdamW
5. **å­¦ä¹ ç‡è°ƒåº¦**: Cosine with warmup
6. **è®­ç»ƒå¾ªç¯**:
   - æ¢¯åº¦ç´¯ç§¯
   - æ··åˆç²¾åº¦è®­ç»ƒ
   - æ¢¯åº¦è£å‰ª
   - æ—¥å¿—è®°å½•

**å¤šGPUæ”¯æŒ**: ä½¿ç”¨ `DistributedDataParallel`

**å¯åŠ¨è„šæœ¬**: `train_multi_gpu.sh`

---

## 8ï¸âƒ£ æµ‹è¯•ä¸éªŒè¯

### 8.1 æµ‹è¯•è„šæœ¬

**ä½ç½®**: `tests/test_full_model.py`

**æµ‹è¯•å†…å®¹**:
1. æ¨¡å‹åˆ›å»ºæµ‹è¯•
2. CPUå‰å‘ä¼ æ’­æµ‹è¯•
3. GPUå‰å‘ä¼ æ’­æµ‹è¯•
4. å‚æ•°ç»Ÿè®¡

**ç¤ºä¾‹è¾“å‡º**:
```
âœ… VRT backbone created
âœ… VRTWithSpike created
Total parameters: 45.23M
Trainable parameters: 45.23M
```

---

### 8.2 å•å…ƒæµ‹è¯•

**ä½ç½®**: `tests/` ç›®å½•

**å¯ç”¨æµ‹è¯•**:
- `test_spike_encoder3d.py`: SpikeEncoder3Dæµ‹è¯•
- `test_fusion.py`: èåˆæ¨¡å—æµ‹è¯•
- `test_dataset.py`: æ•°æ®é›†æµ‹è¯•

**è¿è¡Œæ–¹å¼**:
```bash
pytest tests/
```

---

## 9ï¸âƒ£ å®Œæ•´æ•°æ®æµå›¾

### æ•°æ®æµè¿½è¸ª (å•ä¸ªæ ·æœ¬)

```
[è¾“å…¥é˜¶æ®µ]
æ¨¡ç³Šå¸§ Bâ‚œ: [3, H, W] (RGBå›¾åƒ)
Spikeæµ S: åŸå§‹äº‹ä»¶æµ â†’ å¯¹é½åˆ°[tâ‚€, tâ‚]
    â†“
[ä½“ç´ åŒ–]
S â†’ voxelize() â†’ [K, H, W]  (K=32æ—¶é—´bins)
    â†“ log1p + å½’ä¸€åŒ–
    
[DataLoader batching]
blur:      [B, T, 3, H, W]
spike_vox: [B, T, K, H, W]
    â†“
    
=== VRTWithSpike.forward() ===

[Spikeåˆ†æ”¯]
spike_vox [B,T,K,H,W]
    â†“ SpikeEncoder3D
Fs_1: [B, 96, T, H,   W]     (1x)
Fs_2: [B, 96, T, H/2, W/2]   (1/2x)
Fs_3: [B, 96, T, H/4, W/4]   (1/4x)
Fs_4: [B, 96, T, H/8, W/8]   (1/8x)
    â†“ SpikeTemporalSA
Fs'_1..4 (æ—¶é—´Self-Attentionå)

[RGBåˆ†æ”¯ + èåˆ]
rgb [B,T,3,H,W]
    â†“ VRT.Stage1 (å«TMSA)
Fr_1 [B,96,T,H,W]
    â†“ CrossAttn(Fr_1, Fs'_1)
Ff_1 [B,96,T,H,W]  âœ…èåˆ
    â†“
    â†“ VRT.Stage2
Fr_2 â†’ CrossAttn â†’ Ff_2 âœ…
    â†“ VRT.Stage3
Fr_3 â†’ CrossAttn â†’ Ff_3 âœ…
    â†“ VRT.Stage4
Fr_4 â†’ CrossAttn â†’ Ff_4 âœ…

[è§£ç ]
    â†“ VRT.Stage5 (ç“¶é¢ˆ)
    â†“ VRT.Stage6 + Skip(Ff_3)
    â†“ VRT.Stage7 + Skip(Ff_2)
    â†“ VRT.Stage8 + Skip(Ff_1)
    â†“ LayerNorm
output: [B, T, 3, H, W]

[æŸå¤±è®¡ç®—]
L_char = CharbonnierLoss(output, sharp) * 1.0
L_vgg  = VGGPerceptualLoss(output, sharp) * 0.1
L_total = L_char + L_vgg
```

---

## ğŸ”Ÿ æ¨¡å—æ¸…å•ä¸æ–‡ä»¶å¯¹åº”

| æ ¸éªŒ.mdæ¨¡å— | å®ç°æ–‡ä»¶ | ç±»/å‡½æ•°å | è¡Œå· | çŠ¶æ€ |
|------------|---------|-----------|-----|------|
| **1. è¾“å…¥ä¸æ—¶é—´å¯¹é½** |
| æ¨¡ç³Šå¸§è¾“å…¥ | `src/data/datasets/spike_deblur_dataset.py` | `SpikeDeblurDataset._load_rgb()` | 448-467 | âœ… |
| Spikeæµå¯¹é½ | `src/data/datasets/spike_deblur_dataset.py` | `_load_align_log()` | 311-344 | âœ… |
| **2. Spikeè¡¨å¾è½¬æ¢** |
| ä½“ç´ åŒ– | `src/data/datasets/voxelizer.py` | `voxelize()` | 6-69 | âœ… |
| å½’ä¸€åŒ– | `src/data/datasets/voxelizer.py` | (å†…åµŒåœ¨voxelizeä¸­) | 64-66 | âœ… |
| **3. ç‰¹å¾æå–** |
| VRT RGBç¼–ç å™¨ | `third_party/VRT/models/network_vrt.py` | `VRT` | 1231-1475 | âœ… |
| TMSA (VRTå†…éƒ¨) | `third_party/VRT/models/network_vrt.py` | `TMSA` | 728-1095 | âœ… |
| SpikeEncoder3D | `src/models/spike_encoder3d.py` | `SpikeEncoder3D` | 27-111 | âœ… |
| **4. è§£ç ä¸èåˆ** |
| Spike Self-Attn | `src/models/spike_temporal_sa.py` | `SpikeTemporalSA` | 76-119 | âœ… |
| Cross-Attention | `src/models/fusion/cross_attn_temporal.py` | `MultiScaleTemporalCrossAttnFuse` | 124-152 | âœ… |
| å•å°ºåº¦èåˆå— | `src/models/fusion/cross_attn_temporal.py` | `TemporalCrossAttnFuseBlock` | 63-122 | âœ… |
| VRTè§£ç å™¨ | `third_party/VRT/models/network_vrt.py` | `VRT.forward_features()` | (å¤šå¤„) | âœ… |
| é›†æˆä¸Monkey-patch | `src/models/integrate_vrt.py` | `VRTWithSpike` | 15-219 | âœ… |
| **5. æŸå¤±å‡½æ•°** |
| VGGæ„ŸçŸ¥æŸå¤± | `src/losses/vgg_perceptual.py` | `VGGPerceptualLoss` | 45-79 | âœ… |
| CharbonnieræŸå¤± | `src/losses/charbonnier.py` | `CharbonnierLoss` | 7-21 | âœ… |
| **6. è¾…åŠ©æ¨¡å—** |
| æ³¨æ„åŠ›åˆ†å— | `src/utils/attention.py` | `BaseChunkableAttention` | 61-96 | âœ… |
| LRUç¼“å­˜ | `src/data/datasets/spike_deblur_dataset.py` | `LRUCache` | 104-173 | âœ… |
| å¤šåˆ†è¾¨ç‡è£å‰ª | `src/data/datasets/spike_deblur_dataset.py` | `_apply_crop()` | 528-575 | âœ… |

---

## â“« æ ¸éªŒç»“è®º

### âœ… å®ç°å®Œæ•´æ€§

**æ‰€æœ‰æ ¸éªŒ.mdä¸­æè¿°çš„æ¨¡å—å‡å·²å®ç°**:
- âœ… è¾“å…¥ä¸æ—¶é—´å¯¹é½ (2/2)
- âœ… Spikeè¡¨å¾è½¬æ¢ (2/2)
- âœ… ç‰¹å¾æå– (2/2)
- âœ… è§£ç ä¸èåˆ (4/4)
- âœ… æŸå¤±å‡½æ•° (2/2)

**æ€»è®¡**: 12/12 æ¨¡å—å®Œæˆï¼Œå®Œæˆåº¦ **100%**

---

### ğŸ¯ æ¶æ„ä¸€è‡´æ€§

é¡¹ç›®å®ç°ä¸æ ¸éªŒ.mdæè¿°çš„æ¶æ„**å®Œå…¨ä¸€è‡´**:

1. **æ•°æ®æµè·¯å¾„æ­£ç¡®**:
   - RGBé€šè¿‡VRTç¼–ç å™¨ â†’ TMSA â†’ Fr_i
   - Spikeé€šè¿‡SpikeEncoder3D â†’ Self-Attn â†’ Fs'_i
   - Cross-Attentionèåˆ â†’ Ff_i
   - VRTè§£ç å™¨è¾“å‡º

2. **å½¢çŠ¶å¯¹é½å‡†ç¡®**:
   - 4ä¸ªå°ºåº¦çš„ç©ºé—´åˆ†è¾¨ç‡å¯¹é½ (1x, 1/2x, 1/4x, 1/8x)
   - é€šé“æ•°ä¸€è‡´ (96ç»´)
   - æ—¶é—´ç»´åº¦ä¿æŒ

3. **èåˆä½ç½®æ­£ç¡®**:
   - ä»…åœ¨ç¼–ç ç«¯Stage 1-4èåˆ
   - è§£ç ç«¯ä½¿ç”¨èåˆåçš„ç‰¹å¾åšè·³è¿

---

### ğŸ’¡ å®ç°äº®ç‚¹

1. **å†…å­˜ä¼˜åŒ–**:
   - è‡ªé€‚åº”åˆ†å—æ³¨æ„åŠ›
   - LRUç¼“å­˜æœºåˆ¶
   - Gradient checkpointing

2. **å·¥ç¨‹å®è·µ**:
   - Monkey-patché›†æˆVRT (æ— éœ€ä¿®æ”¹ç¬¬ä¸‰æ–¹ä»£ç )
   - å¤šåˆ†è¾¨ç‡æ•°æ®å¤„ç†
   - å®Œå–„çš„é…ç½®ç³»ç»Ÿ

3. **å¯æ‰©å±•æ€§**:
   - æ¨¡å—åŒ–è®¾è®¡
   - çµæ´»çš„é…ç½®å‚æ•°
   - æ”¯æŒå¤šGPUè®­ç»ƒ

---

### ğŸ“Œ å»ºè®®ä¸æ³¨æ„äº‹é¡¹

1. **VRT DecoderæŸ¥æ‰¾**:
   - è§£ç å™¨é€»è¾‘åˆ†æ•£åœ¨VRTçš„Stage 5-8ä¸­
   - ä¸Šé‡‡æ ·é€šè¿‡ `Upsample3D` æ¨¡å—å®ç°
   - å»ºè®®æŸ¥çœ‹ `third_party/VRT/models/network_vrt.py` ä¸­çš„ `Upsample3D` å’Œ `Stage` ç±»

2. **TMSAè¯¦ç»†ç†è§£**:
   - TMSAæ˜¯VRTçš„æ ¸å¿ƒç»„ä»¶ï¼Œä½äº `network_vrt.py:728-1095`
   - åŒ…å«çª—å£åŒ–æ³¨æ„åŠ›ã€ä½ç½®ç¼–ç ç­‰å¤æ‚æœºåˆ¶
   - å»ºè®®é˜…è¯»VRTåŸè®ºæ–‡ä»¥æ·±å…¥ç†è§£

3. **æ€§èƒ½ç›‘æ§**:
   - ä½¿ç”¨ `attention_utils.py` çš„æ—¥å¿—åŠŸèƒ½ç›‘æ§chunkå¤§å°
   - ä½¿ç”¨ `LRUCache.get_stats()` ç›‘æ§ç¼“å­˜å‘½ä¸­ç‡
   - TensorBoardè®°å½•è®­ç»ƒæŒ‡æ ‡

---

## ğŸ“š å‚è€ƒæ–‡æ¡£

1. **é¡¹ç›®æ–‡æ¡£**:
   - `docs/æ ¸éªŒ.md`: æ¶æ„è¯´æ˜
   - `docs/CHANGES_SUMMARY.md`: å˜æ›´æ€»ç»“
   - `docs/VRT+Spike Baseline å®æ–½è¿›åº¦.md`: å®æ–½è¿›åº¦

2. **é…ç½®æ–‡ä»¶**:
   - `configs/deblur/vrt_spike_baseline.yaml`: ä¸»é…ç½®
   - `third_party/VRT/options/deblur/vrt_base.yaml`: VRTé…ç½®

3. **æµ‹è¯•è„šæœ¬**:
   - `tests/test_full_model.py`: å®Œæ•´æ¨¡å‹æµ‹è¯•
   - `tests/test_spike_encoder3d.py`: ç¼–ç å™¨æµ‹è¯•

---

## ğŸ” é™„å½•: å…³é”®ä»£ç ç‰‡æ®µ

### A. å®Œæ•´å‰å‘ä¼ æ’­

```python
# src/models/integrate_vrt.py:194-218
def forward(self, rgb_clip: torch.Tensor, spike_vox: torch.Tensor) -> torch.Tensor:
    """
    Args:
        rgb_clip: (B, T, 3, H, W) RGB è¾“å…¥åºåˆ—
        spike_vox: (B, T, K, H, W) ä½“ç´ åŒ–çš„ Spike è¾“å…¥
    Returns:
        (B, T, 3, H, W) é‡å»ºçš„æ¸…æ™°å¸§
    """
    # Spike åˆ†æ”¯å¤„ç†
    spike_feats = self.spike_encoder(spike_vox)  # Fs_1..4
    spike_feats_fused = self.spike_temporal_sa(spike_feats)  # Fs'_1..4

    # Monkey-patch VRT çš„ forward_features
    self._monkeypatch_forward_features(spike_feats_fused)
    try:
        out = self.vrt(rgb_clip)
    finally:
        self._restore_forward_features()
    return out
```

---

### B. èåˆé€»è¾‘

```python
# src/models/integrate_vrt.py:96-141
def _fuse_after_stage(i: int, x_stage_out: torch.Tensor) -> torch.Tensor:
    """ç¼–ç ç«¯ Stage è¾“å‡ºåï¼Œä¸ Spike ç‰¹å¾åš Cross-Attention èåˆ"""
    sf = spike_feats_fused[i]  # Fs'_i, [B, C, T, H, W]
    
    # VRTæ ¼å¼: [B, C, D, H, W] â†’ [B, T, C, H, W]
    Fr_btchw = x_stage_out.permute(0, 2, 1, 3, 4)
    
    # Spikeæ ¼å¼: [B, C, T, H, W] â†’ [B, T, C, H, W]
    sf_btchw = sf.permute(0, 2, 1, 3, 4)
    
    # ç©ºé—´å¯¹é½ (å¦‚éœ€è¦)
    if sf_btchw.shape[3:] != Fr_btchw.shape[3:]:
        sf_btchw = F.interpolate(...)
    
    # Cross-Attentionèåˆ
    Ff_btchw = cross_attn_fuse.fuse_blocks[i](Fr_btchw, sf_btchw)
    
    # è½¬æ¢å›VRTæ ¼å¼
    Ff = Ff_btchw.permute(0, 2, 1, 3, 4)  # [B, C, D, H, W]
    return Ff
```

---

### C. ä½“ç´ åŒ–æ ¸å¿ƒ

```python
# src/data/datasets/voxelizer.py:6-69
def voxelize(events, t0, t1, height, width, bins=32, 
             apply_log1p=True, mean=None, std=None):
    vox = np.zeros((bins, height, width), dtype=np.float32)
    duration = float(t1 - t0)
    
    # è®¡ç®—æ—¶é—´binç´¢å¼•
    bin_idx = np.floor((events[:, 0] - t0) / duration * bins).astype(np.int64)
    bin_idx = np.clip(bin_idx, 0, bins - 1)
    
    # ç´¯åŠ äº‹ä»¶åˆ°ä½“ç´ 
    ys = events[:, 1].astype(np.int64)
    xs = events[:, 2].astype(np.int64)
    np.add.at(vox, (bin_idx, ys, xs), 1.0)
    
    # å½’ä¸€åŒ–
    if apply_log1p:
        vox = np.log1p(vox)
    if mean is not None and std is not None:
        vox = (vox - mean) / max(std, 1e-12)
    
    return vox.astype(np.float32)
```

---

## ğŸ“Š ç»Ÿè®¡æ•°æ®

### ä»£ç è§„æ¨¡

| æ¨¡å— | æ–‡ä»¶æ•° | ä»£ç è¡Œæ•°(ä¼°ç®—) |
|-----|-------|-------------|
| æ•°æ®åŠ è½½ | 3 | ~1200 |
| æ¨¡å‹å®ç° | 5 | ~800 |
| æŸå¤±å‡½æ•° | 2 | ~100 |
| VRT (ç¬¬ä¸‰æ–¹) | 1 | ~1500 |
| è®­ç»ƒè„šæœ¬ | 1 | ~500 |
| æµ‹è¯•ä»£ç  | 3 | ~300 |
| **æ€»è®¡** | **15** | **~4400** |

### å‚æ•°é‡

| æ¨¡å— | å‚æ•°é‡(M) |
|-----|----------|
| VRT Backbone | ~35M |
| SpikeEncoder3D | ~5M |
| SpikeTemporalSA | ~2M |
| Cross-Attention | ~3M |
| **æ€»è®¡** | **~45M** |

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-10-15  
**æ ¸éªŒäºº**: AI Assistant  
**çŠ¶æ€**: âœ… æ‰€æœ‰æ¨¡å—å·²å®ç°å¹¶éªŒè¯

