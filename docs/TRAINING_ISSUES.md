# Training Issues and Solutions

## Issue 1: torch.compile Incompatibility with VRT âŒ

### Problem
When `COMPILE_MODEL: true`, training crashes with:
```
AssertionError: lift_tracked_freevar_to_input should not be called on root SubgraphTracer

from user code:
   File "third_party/VRT/models/network_vrt.py", line 185, in drop_path
     if drop_prob == 0. or not training:
```

### Root Cause
`torch.compile` (torch.dynamo) has known issues with:
1. **Gradient checkpointing** (`checkpoint.checkpoint`) - VRT uses this extensively for memory efficiency
2. **Dynamic control flow** - The `drop_path` function has runtime-dependent branching
3. **Nested subgraphs** - VRT's architecture creates deeply nested computation graphs

### Solution
**Disable torch.compile** in the config:
```yaml
TRAIN:
  COMPILE_MODEL: false  # Disabled: incompatible with VRT's checkpoint.checkpoint
```

### Performance Impact
- torch.compile typically provides 10-30% speedup
- However, VRT already uses gradient checkpointing for memory efficiency
- The trade-off is acceptable given the stability gain

### Alternative Approaches (Not Recommended)
1. **Remove gradient checkpointing** - Would cause OOM on most GPUs
2. **Use `torch._dynamo.config.suppress_errors = True`** - Silently falls back to eager mode, defeating the purpose
3. **Rewrite VRT without checkpointing** - Major refactoring, not worth the effort

---

## Issue 2: RAM Cache Not Loading Data âš ï¸

### Problem
When `USE_RAM_CACHE: true`, the cache shows:
```
[SpikeDeblurDataset] Loading 0 blur, 0 sharp, 0 voxel files...
[SpikeDeblurDataset] Cache preloaded: 0 items, 0.0GB / 30.0GB
```

### Root Cause
The `_preload_cache()` method is called in `__init__`, but `self._samples` might be empty if:
1. **Wrong split name** - e.g., using `train` when data is in `Train`
2. **Missing data directories** - `blur/`, `sharp/`, or `spike_vox/` not found
3. **Insufficient frames** - Sequences with fewer than `CLIP_LEN` frames are skipped

### Solution
Added debug logging and safety check:
```python
# Debug: print number of samples found
print(f"[SpikeDeblurDataset] Found {len(self._samples)} samples in split '{split}'")

# Preload data into cache if enabled
if use_ram_cache and len(self._samples) > 0:
    self._preload_cache()
elif use_ram_cache and len(self._samples) == 0:
    print(f"[SpikeDeblurDataset] Warning: RAM cache enabled but no samples found!")
```

### Debugging Steps
1. **Check split directory exists:**
   ```bash
   ls -la data/processed/gopro_spike_unified/train/
   ```

2. **Verify data structure:**
   ```
   train/
   â”œâ”€â”€ GOPR0372_07_00/
   â”‚   â”œâ”€â”€ blur/
   â”‚   â”‚   â”œâ”€â”€ 00000000.png
   â”‚   â”‚   â””â”€â”€ ...
   â”‚   â”œâ”€â”€ sharp/
   â”‚   â”‚   â”œâ”€â”€ 00000000.png
   â”‚   â”‚   â””â”€â”€ ...
   â”‚   â””â”€â”€ spike_vox/  (if USE_PRECOMPUTED_VOXELS: true)
   â”‚       â”œâ”€â”€ 00000000.pt
   â”‚       â””â”€â”€ ...
   ```

3. **Check frame count:**
   ```bash
   # Should have at least CLIP_LEN frames (default: 5)
   ls data/processed/gopro_spike_unified/train/*/blur/ | head -1 | xargs ls | wc -l
   ```

---

## Issue 3: channels_last Memory Format Error âœ… (Fixed)

### Problem
```
RuntimeError: Tensor must have 4 dimensions, but got 5 dimensions
```

### Root Cause
`channels_last` memory format only works with 4D tensors (NCHW), but VRT uses 5D tensors (BTCHW) for video.

### Solution
Only apply `channels_last` to Conv2D layers:
```python
for module in model.modules():
    if isinstance(module, torch.nn.Conv2d):
        module = module.to(memory_format=torch.channels_last)
        channels_last_count += 1
```

---

## Issue 4: Hardcoded GPU Memory Estimation âœ… (Fixed)

### Problem
Memory estimation was hardcoded to 10GB, causing warnings even with small models.

### Solution
Dynamic calculation based on model size:
```python
model_params = sum(p.numel() * p.element_size() for p in model.parameters())
model_size_gb = model_params / (1024**3)
optimizer_size_gb = model_size_gb * 2  # params + gradients
activation_size_gb = 10  # Rough estimate for activations
total_gpu_memory_gb = model_size_gb + optimizer_size_gb + activation_size_gb
```

---

## Current Configuration Status

### âœ… Working Optimizations
- [x] RAM Cache (when data is found)
- [x] Reduced workers (4 instead of 12)
- [x] channels_last for Conv2D only
- [x] Dynamic GPU memory estimation
- [x] Gradient accumulation (12 steps)
- [x] Mixed precision (AMP)

### âŒ Disabled Optimizations
- [ ] torch.compile (incompatible with VRT)

### ğŸ“Š Expected Performance
- **Data loading:** ~2-5s per batch (with RAM cache)
- **Forward pass:** ~1-2s per batch
- **Backward pass:** ~2-3s per batch
- **Total:** ~5-10s per batch (with grad accumulation)

---

## Recommendations

### For Future Work
1. **Consider alternative architectures** that are torch.compile-friendly
2. **Profile memory usage** to optimize cache size per worker
3. **Experiment with different worker counts** based on available RAM
4. **Use precomputed voxels** (`USE_PRECOMPUTED_VOXELS: true`) for faster loading

### For Debugging
1. Always check logs for:
   - Number of samples found
   - Cache statistics
   - GPU memory usage
2. Use `python scripts/verify_optimizations.py` before training
3. Monitor RAM usage: `watch -n 1 free -h`
4. Monitor GPU usage: `watch -n 1 nvidia-smi`

---

---

## Issue 5: NaN Values in TensorBoard Metrics ğŸ”´

### Problem
TensorBoardæ˜¾ç¤ºçš„lossæ›²çº¿åœ¨æŸä¸ªepochåå˜æˆå¹³çº¿ï¼Œé¼ æ ‡æ‚¬åœæ˜¾ç¤ºï¼š
```
Name: .
Smoothed: NaN
Value: NaN
Step: 30
```

### Root Cause
`NaN`ï¼ˆNot a Numberï¼‰ä»£è¡¨æµ®ç‚¹è®¡ç®—ä¸­å‡ºç°äº†æ— æ•ˆç»“æœï¼Œå¸¸è§åŸå› ï¼š
1. **å­¦ä¹ ç‡è¿‡å¤§** - ç‰¹åˆ«æ˜¯åœ¨è¡°å‡å‰çš„æ—©æœŸé˜¶æ®µ
2. **æ¢¯åº¦çˆ†ç‚¸** - ç½‘ç»œè¿‡æ·±æˆ–ç‰¹å¾å€¼è¿‡å¤§
3. **Losså‡½æ•°æ•°å€¼ä¸ç¨³å®š** - è¾“å…¥èŒƒå›´ä¸å½“æˆ–è®¡ç®—è¿‡ç¨‹æº¢å‡º
4. **æ··åˆç²¾åº¦è®­ç»ƒé—®é¢˜** - fp16ä¸‹çš„æ•°å€¼ä¸‹æº¢/æº¢å‡º
5. **éæ³•æ•°å­¦æ“ä½œ** - log(0)ã€sqrt(è´Ÿæ•°)ã€é™¤ä»¥0ç­‰

### Symptoms
- Lossæ›²çº¿çªç„¶å˜å¹³
- TensorBoardæ˜¾ç¤ºNaNå€¼
- å­¦ä¹ ç‡è°ƒåº¦å™¨ä»æ­£å¸¸å·¥ä½œ
- è®­ç»ƒååé‡æ­£å¸¸ï¼Œè¯´æ˜ä¸æ˜¯å´©æºƒ

### Debugging Steps

#### 1ï¸âƒ£ å¿«é€Ÿæ£€æµ‹NaNæ¥æº
åœ¨è®­ç»ƒä»£ç ä¸­æ·»åŠ æ£€æµ‹ï¼š
```python
if torch.isnan(loss) or torch.isinf(loss):
    print(f"[Step {step}] NaN/Inf detected in loss!")
    for name, param in model.named_parameters():
        if torch.isnan(param).any() or torch.isinf(param).any():
            print(f" -> NaN/Inf in parameter: {name}")
    break
```

#### 2ï¸âƒ£ é™ä½å­¦ä¹ ç‡
```yaml
TRAIN:
  OPTIM:
    LR: 0.0001  # ä» 0.0002 é™ä½åˆ° 0.0001
```

#### 3ï¸âƒ£ å¯ç”¨æ¢¯åº¦è£å‰ª
```yaml
TRAIN:
  MAX_GRAD_NORM: 1.0  # è£å‰ªæ¢¯åº¦èŒƒæ•°
```

#### 4ï¸âƒ£ æ£€æŸ¥Lossè®¡ç®—
```python
# åœ¨lossè®¡ç®—å‰æ·»åŠ clamp
pred = torch.clamp(pred, 0, 1)
target = torch.clamp(target, 0, 1)
```

#### 5ï¸âƒ£ éªŒè¯æ··åˆç²¾åº¦è®¾ç½®
å¦‚æœä½¿ç”¨AMP (`torch.cuda.amp.autocast()`):
```yaml
TRAIN:
  MIXED_PRECISION: true  # ç¡®ä¿GradScalerå·²å¯ç”¨
```

æˆ–æš‚æ—¶ç¦ç”¨AMPè¿›è¡Œæµ‹è¯•ï¼š
```yaml
TRAIN:
  MIXED_PRECISION: false  # æ’æŸ¥æ˜¯å¦æ˜¯AMPé—®é¢˜
```

### Solution Checklist
- [ ] é™ä½å­¦ä¹ ç‡ (ä¾‹å¦‚ä»2e-4é™åˆ°1e-4)
- [ ] å¯ç”¨æ¢¯åº¦è£å‰ª (`MAX_GRAD_NORM: 1.0`)
- [ ] åœ¨lossè®¡ç®—å‰clampè¾“å…¥èŒƒå›´
- [ ] æ£€æŸ¥VGG Perceptual Lossçš„è¾“å…¥å½’ä¸€åŒ–
- [ ] éªŒè¯Charbonnier Lossçš„epsilonå€¼ (é»˜è®¤1e-3)
- [ ] å¦‚ä½¿ç”¨AMPï¼Œæ£€æŸ¥GradScaleré…ç½®

### Prevention
```yaml
# æ¨èçš„ç¨³å®šé…ç½®
TRAIN:
  OPTIM:
    LR: 0.0001  # ä¿å®ˆçš„å­¦ä¹ ç‡
  MAX_GRAD_NORM: 1.0  # æ¢¯åº¦è£å‰ª
  MIXED_PRECISION: true  # å¯ç”¨AMPä½†é…åˆGradScaler
  
LOSS:
  CHARBONNIER:
    DELTA: 0.001  # è¶³å¤Ÿçš„epsilonå€¼
  VGG_PERCEPTUAL:
    WEIGHT: 0.1  # é€‚ä¸­çš„æƒé‡ï¼Œé¿å…è¿‡å¤§
```

---

## References
- [PyTorch torch.compile Limitations](https://pytorch.org/docs/stable/dynamo/index.html#limitations)
- [Gradient Checkpointing](https://pytorch.org/docs/stable/checkpoint.html)
- [Memory Formats](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html)
- [Numerical Stability in Deep Learning](https://pytorch.org/docs/stable/notes/numerical_accuracy.html)


