# VRT åˆ†å¸ƒå¼è®­ç»ƒæŒ‡å—

## æ¦‚è¿°

æœ¬é¡¹ç›®å·²å®Œæˆç°ä»£åŒ–åˆ†å¸ƒå¼è®­ç»ƒæ”¹é€ ï¼Œæ”¯æŒä»¥ä¸‹ä¸¤ç§åœºæ™¯ï¼š

1. **å¹³å° DDP è®­ç»ƒ**ï¼šäº‘å¹³å°/é›†ç¾¤è‡ªåŠ¨æ³¨å…¥ç¯å¢ƒå˜é‡
2. **æœ¬åœ°å¤šå¡è®­ç»ƒ**ï¼šä½¿ç”¨ `torchrun` æˆ–å•å¡è®­ç»ƒ

æ ¸å¿ƒç‰¹æ€§ï¼š
- âœ… è‡ªåŠ¨æ£€æµ‹åˆ†å¸ƒå¼æ¨¡å¼ï¼ˆåŸºäº `WORLD_SIZE` ç¯å¢ƒå˜é‡ï¼‰
- âœ… æ”¯æŒ PyTorch åŸç”Ÿ `env://` åˆå§‹åŒ–æ–¹å¼
- âœ… å…¼å®¹ SLURM é›†ç¾¤ç¯å¢ƒ
- âœ… æ­£ç¡®çš„è®¾å¤‡åˆ†é…ï¼ˆåŸºäº `LOCAL_RANK`ï¼‰
- âœ… è‡ªåŠ¨æ•°æ®åˆ†ç‰‡ï¼ˆDistributedSamplerï¼Œè®­ç»ƒå’ŒéªŒè¯ï¼‰
- âœ… è®­ç»ƒå’ŒéªŒè¯çš„æ‰¹æ¬¡å¤§å°ã€å·¥ä½œè¿›ç¨‹æ•°è‡ªåŠ¨æŒ‰ GPU æ•°é‡åˆ†é…
- âœ… éªŒè¯/æµ‹è¯•æ—¶è‡ªåŠ¨èšåˆæ‰€æœ‰ GPU çš„æŒ‡æ ‡ï¼ˆall_reduceï¼‰
- âœ… ä»…åœ¨ä¸»è¿›ç¨‹ä¿å­˜æ¨¡å‹å’Œæ—¥å¿—
- âœ… æ¨¡å‹ä¿å­˜ä½¿ç”¨åŸå­å†™å…¥ï¼Œç¡®ä¿æ–‡ä»¶å®Œæ•´æ€§

---

## å¿«é€Ÿå¼€å§‹

### åœºæ™¯ä¸€ï¼šå¹³å° DDP è®­ç»ƒ

**é€‚ç”¨æƒ…å†µ**ï¼šäº‘å¹³å°ï¼ˆå¦‚é˜¿é‡Œäº‘ã€è…¾è®¯äº‘ç­‰ï¼‰å·²ä¸ºæ¯ä¸ªè¿›ç¨‹è‡ªåŠ¨æ³¨å…¥ç¯å¢ƒå˜é‡ã€‚

å¹³å°ä¼šè®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š
- `RANK`: å…¨å±€è¿›ç¨‹åºå·
- `LOCAL_RANK`: èŠ‚ç‚¹å†…è¿›ç¨‹åºå·
- `WORLD_SIZE`: æ€»è¿›ç¨‹æ•°
- `MASTER_ADDR`: ä¸»èŠ‚ç‚¹åœ°å€
- `MASTER_PORT`: ä¸»èŠ‚ç‚¹ç«¯å£

**è¿è¡Œå‘½ä»¤**ï¼š

```bash
python -u main_train_vrt.py --opt options/vrt/006_train_vrt_videodeblurring_gopro_rgbspike.json
```

âš ï¸ **é‡è¦**ï¼šä¸è¦ä½¿ç”¨ `torchrun`ï¼å¹³å°å·²ç»ä¸ºæ¯ä¸ªè¿›ç¨‹è¿è¡Œäº†ç›¸åŒçš„å‘½ä»¤ï¼Œä½¿ç”¨ `torchrun` ä¼šå¯¼è‡´åµŒå¥—å¤šè¿›ç¨‹ã€‚

---

### åœºæ™¯äºŒï¼šæœ¬åœ°å¤šå¡è®­ç»ƒ

**é€‚ç”¨æƒ…å†µ**ï¼šåœ¨æœ¬åœ°æœºå™¨æˆ–æœªé…ç½®ç¯å¢ƒå˜é‡çš„æœåŠ¡å™¨ä¸Šè®­ç»ƒã€‚

#### æ–¹å¼ 1ï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# å•å¡è®­ç»ƒ
./launch_train.sh 1

# 4 å¡è®­ç»ƒ
./launch_train.sh 4

# 8 å¡è®­ç»ƒï¼Œä½¿ç”¨è‡ªå®šä¹‰é…ç½®
./launch_train.sh 8 options/vrt/006_train_vrt_videodeblurring_gopro_rgbspike.json
```

#### æ–¹å¼ 2ï¼šç›´æ¥ä½¿ç”¨ torchrun

```bash
# 4 å¡è®­ç»ƒ
torchrun --nproc_per_node=4 main_train_vrt.py \
    --opt options/vrt/006_train_vrt_videodeblurring_gopro_rgbspike.json

# 8 å¡è®­ç»ƒ
torchrun --nproc_per_node=8 main_train_vrt.py \
    --opt options/vrt/006_train_vrt_videodeblurring_gopro_rgbspike.json
```

#### æ–¹å¼ 3ï¼šå•å¡è®­ç»ƒ

```bash
python main_train_vrt.py --opt options/vrt/006_train_vrt_videodeblurring_gopro_rgbspike.json
```

---

## é…ç½®æ–‡ä»¶è¯´æ˜

### åˆ†å¸ƒå¼ç›¸å…³é…ç½®é¡¹

åœ¨ JSON é…ç½®æ–‡ä»¶ä¸­ï¼ˆå¦‚ `006_train_vrt_videodeblurring_gopro_rgbspike.json`ï¼‰ï¼š

```json
{
  "gpu_ids": [0,1,2],
  "dist": true,
  "find_unused_parameters": false,
  "use_static_graph": true
}
```

#### `gpu_ids` å­—æ®µ

- **å•è¿›ç¨‹æ¨¡å¼**ï¼šæŒ‡å®šä½¿ç”¨å“ªäº› GPUï¼ˆä¾‹å¦‚ `[0,1,2]` è¡¨ç¤ºä½¿ç”¨ 0ã€1ã€2 å·å¡ï¼‰
- **åˆ†å¸ƒå¼æ¨¡å¼**ï¼šæ­¤å­—æ®µè¢«å¿½ç•¥ï¼Œè®¾å¤‡ç”± `torchrun` æˆ–å¹³å°è‡ªåŠ¨åˆ†é…
- **å•å¡è®­ç»ƒ**ï¼šè®¾ç½®ä¸º `[0]`

ç¨‹åºä¼šè‡ªåŠ¨è®¾ç½® `CUDA_VISIBLE_DEVICES` ç¯å¢ƒå˜é‡ï¼ˆä»…åœ¨å•è¿›ç¨‹æ¨¡å¼ä¸‹ï¼‰ã€‚

#### `dist` å­—æ®µ

- è‡ªåŠ¨ä» `WORLD_SIZE` ç¯å¢ƒå˜é‡æ£€æµ‹
- æ‰‹åŠ¨è®¾ç½®ä¼šè¢«è‡ªåŠ¨æ£€æµ‹ç»“æœè¦†ç›–
- ä¿ç•™æ­¤å­—æ®µæ˜¯ä¸ºäº†æ–‡æ¡£ç›®çš„

#### `find_unused_parameters` å­—æ®µ

- DDP ç›¸å…³å‚æ•°ï¼Œæ§åˆ¶æ˜¯å¦æŸ¥æ‰¾æœªä½¿ç”¨çš„æ¨¡å‹å‚æ•°
- VRT æ¨¡å‹è®¾ç½®ä¸º `false` å¯æå‡æ€§èƒ½

#### `use_static_graph` å­—æ®µ

- å¯ç”¨é™æ€è®¡ç®—å›¾ä¼˜åŒ–ï¼ˆPyTorch >= 1.11ï¼‰
- å¯¹å›ºå®šç½‘ç»œç»“æ„çš„æ¨¡å‹å¯æå‡è®­ç»ƒé€Ÿåº¦

---

## ç¯å¢ƒå˜é‡

### è‡ªåŠ¨è®¾ç½®çš„ç¯å¢ƒå˜é‡

åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œä»¥ä¸‹ç¯å¢ƒå˜é‡ç”± `torchrun` æˆ–å¹³å°è‡ªåŠ¨è®¾ç½®ï¼š

| ç¯å¢ƒå˜é‡ | è¯´æ˜ | ç¤ºä¾‹å€¼ |
|---------|------|--------|
| `RANK` | å…¨å±€è¿›ç¨‹åºå·ï¼ˆ0 åˆ° world_size-1ï¼‰ | `0`, `1`, `2`, ... |
| `LOCAL_RANK` | å•èŠ‚ç‚¹å†…è¿›ç¨‹åºå· | `0`, `1`, `2`, ... |
| `WORLD_SIZE` | æ€»è¿›ç¨‹æ•°ï¼ˆç­‰äº GPU æ•°é‡ï¼‰ | `4`, `8` |
| `MASTER_ADDR` | ä¸»èŠ‚ç‚¹åœ°å€ | `localhost`, `192.168.1.100` |
| `MASTER_PORT` | ä¸»èŠ‚ç‚¹ç«¯å£ | `29500` |

### æ¨èçš„ NCCL ç¯å¢ƒå˜é‡

ä¸ºäº†æå‡è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ï¼Œå»ºè®®è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

```bash
# å¼‚æ­¥é”™è¯¯å¤„ç†ï¼ˆæ¨èï¼‰
export NCCL_ASYNC_ERROR_HANDLING=1

# å¦‚æœæ²¡æœ‰ InfiniBand ç½‘ç»œï¼Œç¦ç”¨ IB
export NCCL_IB_DISABLE=1

# é™åˆ¶ CUDA è¿æ¥æ•°ï¼ˆæŸäº›æ¨¡å‹å¯æå‡ç¨³å®šæ€§ï¼‰
export CUDA_DEVICE_MAX_CONNECTIONS=1

# å¯ç”¨ NCCL è°ƒè¯•ä¿¡æ¯ï¼ˆè°ƒè¯•æ—¶ä½¿ç”¨ï¼‰
# export NCCL_DEBUG=INFO
```

å¯ä»¥å°†è¿™äº›å˜é‡æ·»åŠ åˆ° `~/.bashrc` æˆ–åœ¨è¿è¡Œè®­ç»ƒå‰å¯¼å‡ºï¼š

```bash
export NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_IB_DISABLE=1
./launch_train.sh 4
```

---

## å·¥ä½œåŸç†

### è‡ªåŠ¨æ£€æµ‹æœºåˆ¶

ç¨‹åºå¯åŠ¨æ—¶æŒ‰ä»¥ä¸‹é¡ºåºæ£€æµ‹åˆ†å¸ƒå¼æ¨¡å¼ï¼š

1. æ£€æŸ¥ `WORLD_SIZE` ç¯å¢ƒå˜é‡
   - å¦‚æœ `WORLD_SIZE > 1`ï¼šå¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
   - å¦‚æœ `WORLD_SIZE` ä¸å­˜åœ¨æˆ–ç­‰äº 1ï¼šå•è¿›ç¨‹æ¨¡å¼

2. è¯»å– rank ä¿¡æ¯ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š
   - `LOCAL_RANK` ç¯å¢ƒå˜é‡ï¼ˆtorchrun æ ‡å‡†ï¼‰
   - `SLURM_LOCALID`ï¼ˆSLURM é›†ç¾¤ï¼‰
   - é»˜è®¤å€¼ï¼š0

3. è®¾ç½® CUDA è®¾å¤‡ï¼š
   ```python
   torch.cuda.set_device(local_rank)
   device = torch.device(f"cuda:{local_rank}")
   ```

4. åˆå§‹åŒ–è¿›ç¨‹ç»„ï¼š
   ```python
   torch.distributed.init_process_group(
       backend='nccl',
       init_method='env://'
   )
   ```

### æ•°æ®åŠ è½½

#### è®­ç»ƒæ•°æ®åŠ è½½

åœ¨åˆ†å¸ƒå¼æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨ `DistributedSampler` è‡ªåŠ¨åˆ†ç‰‡è®­ç»ƒæ•°æ®ï¼š

```python
from torch.utils.data.distributed import DistributedSampler

# åˆ›å»ºè®­ç»ƒ sampler
train_sampler = DistributedSampler(
    train_dataset,
    shuffle=True,
    drop_last=True,
    seed=seed
)

# åˆ›å»ºè®­ç»ƒ DataLoader
# æ‰¹æ¬¡å¤§å°å’Œå·¥ä½œè¿›ç¨‹æ•°ä¼šè‡ªåŠ¨æŒ‰ GPU æ•°é‡åˆ†é…
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size // num_gpu,  # æ¯å¡æ‰¹æ¬¡å¤§å°
    shuffle=False,  # åˆ†å¸ƒå¼æ—¶ç”± sampler æ§åˆ¶
    num_workers=num_workers // num_gpu,  # æ¯å¡å·¥ä½œè¿›ç¨‹æ•°
    drop_last=True,
    pin_memory=True,
    sampler=train_sampler
)

# æ¯ä¸ª epoch å¼€å§‹æ—¶è®¾ç½® epochï¼ˆç¡®ä¿æ•°æ®éšæœºæ€§ï¼‰
for epoch in range(num_epochs):
    train_sampler.set_epoch(epoch)
    for batch in train_loader:
        # è®­ç»ƒä»£ç 
        ...
```

#### éªŒè¯/æµ‹è¯•æ•°æ®åŠ è½½

éªŒè¯å’Œæµ‹è¯•æ—¶åŒæ ·ä½¿ç”¨ `DistributedSampler` è¿›è¡Œæ•°æ®åˆ†ç‰‡ï¼š

```python
# åˆ›å»ºæµ‹è¯• sampler
test_sampler = DistributedSampler(
    test_dataset,
    shuffle=False,  # éªŒè¯æ—¶é€šå¸¸ä¸æ‰“ä¹±
    drop_last=False,  # ä¿ç•™æ‰€æœ‰æ•°æ®
    seed=seed
)

# åˆ›å»ºæµ‹è¯• DataLoader
test_loader = DataLoader(
    test_dataset,
    batch_size=max(1, test_batch_size // num_gpu),  # æ¯å¡æ‰¹æ¬¡å¤§å°
    shuffle=False,
    num_workers=max(1, test_num_workers // num_gpu),  # æ¯å¡å·¥ä½œè¿›ç¨‹æ•°
    drop_last=False,
    pin_memory=True,
    sampler=test_sampler
)
```

**é‡è¦è¯´æ˜**ï¼š
- è®­ç»ƒå’ŒéªŒè¯çš„æ‰¹æ¬¡å¤§å°ã€å·¥ä½œè¿›ç¨‹æ•°éƒ½ä¼šè‡ªåŠ¨æŒ‰ GPU æ•°é‡åˆ†é…
- è®­ç»ƒæ—¶æ¯ä¸ª epoch éœ€è¦è°ƒç”¨ `sampler.set_epoch(epoch)` ç¡®ä¿æ•°æ®éšæœºæ€§
- éªŒè¯æ—¶é€šå¸¸ä¸éœ€è¦è®¾ç½® epochï¼ˆå› ä¸º `shuffle=False`ï¼‰

### æ¨¡å‹å°è£…

æ¨¡å‹è‡ªåŠ¨ä½¿ç”¨ `DistributedDataParallel` å°è£…ï¼š

```python
model = DistributedDataParallel(
    model,
    device_ids=[local_rank],
    output_device=local_rank,
    broadcast_buffers=False,  # æ€§èƒ½ä¼˜åŒ–
    find_unused_parameters=opt['find_unused_parameters']
)
```

### æ—¥å¿—å’Œä¿å­˜

**é‡è¦åŸåˆ™**ï¼šåªåœ¨ä¸»è¿›ç¨‹ï¼ˆrank 0ï¼‰æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

- âœ… åˆ›å»ºæ—¥å¿—ç›®å½•
- âœ… ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
- âœ… ä¿å­˜è®­ç»ƒçŠ¶æ€
- âœ… å†™å…¥ TensorBoard/W&B æ—¥å¿—
- âœ… æ‰“å°è®­ç»ƒä¿¡æ¯

ä»£ç ç¤ºä¾‹ï¼š

```python
from utils.utils_dist import is_main_process, barrier_safe

# æ‰“å°æ—¥å¿—ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
if opt['rank'] == 0:
    logger.info(f"Epoch {epoch}, Loss: {loss:.4f}")

# ä¿å­˜æ¨¡å‹ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
if current_step % checkpoint_save == 0 and opt['rank'] == 0:
    logger.info('Saving the model.')
    model.save(current_step)  # å†…éƒ¨å·²ä½¿ç”¨ is_main_process() æ£€æŸ¥

# åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œç­‰å¾… rank 0 å®Œæˆä¿å­˜
if current_step % checkpoint_save == 0 and opt['dist']:
    barrier_safe()  # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
```

**æ¨¡å‹ä¿å­˜å®ç°ç»†èŠ‚**ï¼š
- æ¨¡å‹ä¿å­˜æ–¹æ³•å†…éƒ¨ä½¿ç”¨ `is_main_process()` æ£€æŸ¥ï¼Œç¡®ä¿åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜
- ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶å®ç°åŸå­å†™å…¥ï¼Œé¿å…ä¿å­˜è¿‡ç¨‹ä¸­æ–‡ä»¶æŸå
- ä¿å­˜åä½¿ç”¨ `barrier_safe()` åŒæ­¥æ‰€æœ‰è¿›ç¨‹ï¼Œç¡®ä¿çŠ¶æ€ä¸€è‡´

### éªŒè¯/æµ‹è¯•æ—¶çš„æŒ‡æ ‡èšåˆ

åœ¨åˆ†å¸ƒå¼éªŒè¯/æµ‹è¯•æ—¶ï¼Œæ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸åŒçš„æ•°æ®å­é›†ï¼Œéœ€è¦èšåˆæ‰€æœ‰è¿›ç¨‹çš„æŒ‡æ ‡ï¼š

```python
import torch.distributed as dist

# æ¯ä¸ªè¿›ç¨‹è®¡ç®—æœ¬åœ°æŒ‡æ ‡
local_psnr_sum = sum(test_results['psnr'])
local_psnr_count = len(test_results['psnr'])

# åˆ›å»ºå¼ é‡ç”¨äºèšåˆ
metrics_tensor = torch.tensor(
    [local_psnr_sum, local_psnr_count],
    dtype=torch.float64,
    device=device
)

# ä½¿ç”¨ all_reduce èšåˆæ‰€æœ‰è¿›ç¨‹çš„æŒ‡æ ‡
if opt['dist']:
    dist.all_reduce(metrics_tensor, op=dist.ReduceOp.SUM)

# è®¡ç®—å…¨å±€å¹³å‡å€¼
global_psnr_sum, global_psnr_count = metrics_tensor.tolist()
ave_psnr = global_psnr_sum / global_psnr_count

# åªåœ¨ä¸»è¿›ç¨‹æ‰“å°ç»“æœ
if is_main_process():
    logger.info(f'Average PSNR: {ave_psnr:.2f} dB')
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨ `dist.all_reduce()` èšåˆæ‰€æœ‰ GPU çš„æŒ‡æ ‡ï¼ˆsum æ“ä½œï¼‰
- èšåˆåè®¡ç®—å…¨å±€å¹³å‡å€¼ï¼ˆsum / countï¼‰
- åªåœ¨ä¸»è¿›ç¨‹æ‰“å°å’Œè®°å½•æœ€ç»ˆç»“æœ
- éªŒè¯å‰ä½¿ç”¨ `barrier_safe()` ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åŒæ­¥

---

## å¸¸è§é—®é¢˜

### 1. ä¸ºä»€ä¹ˆä¸èƒ½åœ¨å¹³å° DDP ä¸­ä½¿ç”¨ torchrunï¼Ÿ

**åŸå› **ï¼šå¹³å°å·²ç»ä¸ºæ¯ä¸ª GPU å¯åŠ¨äº†ä¸€ä¸ªç‹¬ç«‹çš„è¿›ç¨‹ï¼Œå¹¶ä¸ºæ¯ä¸ªè¿›ç¨‹è®¾ç½®äº†æ­£ç¡®çš„ç¯å¢ƒå˜é‡ã€‚å¦‚æœå†ä½¿ç”¨ `torchrun`ï¼Œä¼šå¯¼è‡´æ¯ä¸ªè¿›ç¨‹åˆåˆ›å»ºå¤šä¸ªå­è¿›ç¨‹ï¼Œé€ æˆæ··ä¹±ã€‚

**æ­£ç¡®åšæ³•**ï¼š
```bash
# å¹³å° DDPï¼ˆæ¯ä¸ªè¿›ç¨‹è¿è¡Œç›¸åŒå‘½ä»¤ï¼‰
python -u main_train_vrt.py --opt config.json
```

**é”™è¯¯åšæ³•**ï¼š
```bash
# âŒ ä¼šå¯¼è‡´åµŒå¥—å¤šè¿›ç¨‹
torchrun --nproc_per_node=4 main_train_vrt.py --opt config.json
```

### 2. å¦‚ä½•ç¡®è®¤åˆ†å¸ƒå¼è®­ç»ƒæ­£å¸¸å·¥ä½œï¼Ÿ

æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼Œåº”è¯¥çœ‹åˆ°ï¼š

```
========================================
Distributed Training Setup
========================================
Backend: nccl
World Size: 4
Rank: 0
Local Rank: 0
Master: localhost:29500
========================================
```

æ¯ä¸ªè¿›ç¨‹ä¼šè¾“å‡ºè‡ªå·±çš„ rank ä¿¡æ¯ã€‚åªæœ‰ rank 0 ä¼šä¿å­˜æ¨¡å‹å’Œè¾“å‡ºè¯¦ç»†æ—¥å¿—ã€‚

### 3. CUDA out of memory é”™è¯¯

**åŸå› **ï¼šæ‰¹æ¬¡å¤§å°ï¼ˆbatch sizeï¼‰éœ€è¦æ ¹æ® GPU æ•°é‡è°ƒæ•´ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

- **è®­ç»ƒæ—¶**ï¼šæ¯å¼ å¡çš„æ‰¹æ¬¡å¤§å° = é…ç½®ä¸­çš„ `dataloader_batch_size // num_gpu`
- **éªŒè¯æ—¶**ï¼šæ¯å¼ å¡çš„æ‰¹æ¬¡å¤§å° = `max(1, dataloader_batch_size // num_gpu)`
- **æ€»æ‰¹æ¬¡å¤§å°** = æ¯å¡æ‰¹æ¬¡å¤§å° Ã— å¡æ•°

ä¾‹å¦‚ï¼ˆ4 å¡è®­ç»ƒï¼‰ï¼š
- é…ç½®ä¸­ `dataloader_batch_size = 8`
- è®­ç»ƒæ—¶ï¼šæ¯å¡ `batch_size = 8 // 4 = 2`ï¼ˆæ€»æ‰¹æ¬¡ = 2 Ã— 4 = 8ï¼‰
- éªŒè¯æ—¶ï¼šæ¯å¡ `batch_size = max(1, 1 // 4) = 1`ï¼ˆæ€»æ‰¹æ¬¡ = 1 Ã— 4 = 4ï¼‰

åœ¨é…ç½®æ–‡ä»¶ä¸­è°ƒæ•´ï¼š

```json
{
  "datasets": {
    "train": {
      "dataloader_batch_size": 2,  // æ¯å¡æ‰¹æ¬¡å¤§å°ï¼ˆä¼šè‡ªåŠ¨é™¤ä»¥ GPU æ•°ï¼‰
      "dataloader_num_workers": 8  // æ¯å¡å·¥ä½œè¿›ç¨‹æ•°ï¼ˆä¼šè‡ªåŠ¨é™¤ä»¥ GPU æ•°ï¼‰
    },
    "test": {
      "dataloader_batch_size": 1,  // æ¯å¡æ‰¹æ¬¡å¤§å°ï¼ˆä¼šè‡ªåŠ¨é™¤ä»¥ GPU æ•°ï¼‰
      "dataloader_num_workers": 8, // æ¯å¡å·¥ä½œè¿›ç¨‹æ•°ï¼ˆä¼šè‡ªåŠ¨é™¤ä»¥ GPU æ•°ï¼‰
      "dataloader_shuffle": false
    }
  }
}
```

> **é‡è¦æç¤º**ï¼š
> - è®­ç»ƒå’ŒéªŒè¯çš„ `dataloader_batch_size` å’Œ `dataloader_num_workers` éƒ½ä¼š**è‡ªåŠ¨æŒ‰ GPU æ•°é‡åˆ†é…**
> - é…ç½®æ–‡ä»¶ä¸­å¡«å†™çš„æ˜¯**æ€»æ‰¹æ¬¡å¤§å°**ï¼Œç¨‹åºä¼šè‡ªåŠ¨è®¡ç®—æ¯å¡çš„æ‰¹æ¬¡å¤§å°
> - éªŒè¯æ—¶ä½¿ç”¨ `max(1, ...)` ç¡®ä¿æ¯å¡è‡³å°‘å¤„ç† 1 ä¸ªæ ·æœ¬

### 4. NCCL åˆå§‹åŒ–è¶…æ—¶

**å¸¸è§åŸå› **ï¼š

1. é˜²ç«å¢™é˜»æ­¢è¿›ç¨‹é—´é€šä¿¡
2. `MASTER_ADDR` æˆ– `MASTER_PORT` è®¾ç½®é”™è¯¯
3. ç½‘ç»œé…ç½®é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# å¢åŠ è¶…æ—¶æ—¶é—´
export NCCL_TIMEOUT=1800

# å¯ç”¨è°ƒè¯•ä¿¡æ¯
export NCCL_DEBUG=INFO

# æŒ‡å®šç½‘ç»œæ¥å£ï¼ˆå¦‚æœæœ‰å¤šä¸ªç½‘å¡ï¼‰
export NCCL_SOCKET_IFNAME=eth0

# é‡æ–°è¿è¡Œè®­ç»ƒ
./launch_train.sh 4
```

### 5. ä¸åŒè¿›ç¨‹çš„æŸå¤±å€¼ä¸åŒæ­¥

è¿™æ˜¯æ­£å¸¸ç°è±¡ï¼æ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸åŒçš„æ•°æ®åˆ†ç‰‡ï¼Œå› æ­¤ï¼š

- âœ… æ¯ä¸ª GPU çš„æŸå¤±å€¼å¯èƒ½ä¸åŒï¼ˆæ•°æ®ä¸åŒï¼‰
- âœ… æ¢¯åº¦ä¼šåœ¨åå‘ä¼ æ’­æ—¶è‡ªåŠ¨åŒæ­¥
- âœ… æ¨¡å‹å‚æ•°åœ¨æ‰€æœ‰ GPU ä¸Šä¿æŒä¸€è‡´

å¦‚éœ€åŒæ­¥æŒ‡æ ‡ç”¨äºæ—¥å¿—è®°å½•ï¼Œå¯ä»¥ä½¿ç”¨ï¼š

```python
from utils.utils_dist import all_reduce_mean

# è®¡ç®—æ‰€æœ‰è¿›ç¨‹çš„å¹³å‡æŸå¤±
avg_loss = all_reduce_mean(loss_tensor)
```

æˆ–è€…åœ¨éªŒè¯æ—¶ç›´æ¥ä½¿ç”¨ `dist.all_reduce()`ï¼š

```python
import torch.distributed as dist

# èšåˆæ‰€æœ‰è¿›ç¨‹çš„æŒ‡æ ‡
metrics_tensor = torch.tensor([local_sum, local_count], device=device)
if opt['dist']:
    dist.all_reduce(metrics_tensor, op=dist.ReduceOp.SUM)

# è®¡ç®—å…¨å±€å¹³å‡å€¼
global_avg = metrics_tensor[0] / metrics_tensor[1]
```

### 6. è®­ç»ƒé€Ÿåº¦æ²¡æœ‰çº¿æ€§æå‡

**æ­£å¸¸æƒ…å†µ**ï¼š

- 2 å¡ç†è®ºä¸Šåº”è¯¥å¿« 2 å€ï¼Œä½†å®é™…çº¦ 1.7-1.9 å€
- 4 å¡ç†è®ºä¸Šåº”è¯¥å¿« 4 å€ï¼Œä½†å®é™…çº¦ 3.2-3.6 å€
- 8 å¡ç†è®ºä¸Šåº”è¯¥å¿« 8 å€ï¼Œä½†å®é™…çº¦ 6-7 å€

**åŸå› **ï¼š

1. é€šä¿¡å¼€é”€ï¼ˆæ¢¯åº¦åŒæ­¥ï¼‰
2. I/O ç“¶é¢ˆï¼ˆæ•°æ®åŠ è½½ï¼‰
3. è´Ÿè½½ä¸å‡è¡¡

**ä¼˜åŒ–å»ºè®®**ï¼š

```json
{
  "datasets": {
    "train": {
      "dataloader_num_workers": 8,  // å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹
      "dataloader_batch_size": 4    // å¢å¤§æ‰¹æ¬¡å‡å°‘é€šä¿¡é¢‘ç‡
    }
  }
}
```

---

## SLURM é›†ç¾¤æ”¯æŒ

æœ¬é¡¹ç›®å…¼å®¹ SLURM ä½œä¸šè°ƒåº¦ç³»ç»Ÿã€‚

### SLURM ä»»åŠ¡è„šæœ¬ç¤ºä¾‹

åˆ›å»º `submit_job.sh`ï¼š

```bash
#!/bin/bash
#SBATCH --job-name=vrt_train
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=4
#SBATCH --gres=gpu:4
#SBATCH --time=48:00:00
#SBATCH --output=logs/slurm_%j.out
#SBATCH --error=logs/slurm_%j.err

# åŠ è½½ç¯å¢ƒ
module load cuda/11.8
module load pytorch/2.0

# NCCL é…ç½®
export NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_IB_DISABLE=1

# è¿è¡Œè®­ç»ƒï¼ˆSLURM ä¼šè‡ªåŠ¨è®¾ç½®ç¯å¢ƒå˜é‡ï¼‰
srun python -u main_train_vrt.py \
    --opt options/vrt/006_train_vrt_videodeblurring_gopro_rgbspike.json
```

æäº¤ä»»åŠ¡ï¼š

```bash
sbatch submit_job.sh
```

SLURM ä¼šè‡ªåŠ¨è®¾ç½®ï¼š
- `SLURM_PROCID` â†’ æ˜ å°„åˆ° `RANK`
- `SLURM_LOCALID` â†’ æ˜ å°„åˆ° `LOCAL_RANK`
- `SLURM_NTASKS` â†’ æ˜ å°„åˆ° `WORLD_SIZE`

---

## ç›‘æ§è®­ç»ƒ

### TensorBoard

è®­ç»ƒæ—¥å¿—è‡ªåŠ¨ä¿å­˜åˆ° `experiments/<task_name>/tb_logger/`ï¼š

```bash
tensorboard --logdir experiments/006_train_vrt_videodeblurring_gopro_rgbspike/tb_logger
```

### Weights & Biases

å¦‚æœé…ç½®äº† W&Bï¼š

```json
{
  "logging": {
    "use_wandb": true,
    "wandb_project": "VRT-VideoDeblurring",
    "wandb_api_key": "your_api_key"
  }
}
```

è®­ç»ƒä¼šè‡ªåŠ¨ä¸Šä¼ åˆ° W&B å¹³å°ã€‚

### å‘½ä»¤è¡Œè¾“å‡º

åªæœ‰ä¸»è¿›ç¨‹ï¼ˆrank 0ï¼‰ä¼šè¾“å‡ºè¯¦ç»†ä¿¡æ¯ï¼š

```
[2025-11-06 10:00:00] Epoch: 1, Iter: 100, Loss: 0.0234
[2025-11-06 10:05:00] Epoch: 1, Iter: 200, Loss: 0.0198
```

å…¶ä»–è¿›ç¨‹ä¼šä¿æŒé™é»˜æˆ–åªè¾“å‡ºå…³é”®ä¿¡æ¯ã€‚

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ‰¹æ¬¡å¤§å°è°ƒä¼˜

```json
{
  "datasets": {
    "train": {
      "dataloader_batch_size": 4  // æ¯å¡æ‰¹æ¬¡ï¼Œæ ¹æ®æ˜¾å­˜è°ƒæ•´
    }
  }
}
```

**å»ºè®®**ï¼š
- A100 80GB: batch_size = 8-16
- V100 32GB: batch_size = 4-8
- RTX 3090 24GB: batch_size = 2-4

### 2. æ•°æ®åŠ è½½ä¼˜åŒ–

```json
{
  "datasets": {
    "train": {
      "dataloader_num_workers": 8  // CPU æ ¸å¿ƒæ•°çš„ä¸€åŠ
    }
  }
}
```

### 3. æ··åˆç²¾åº¦è®­ç»ƒ

åœ¨ `utils_option.py` ä¸­å¯ç”¨ï¼š

```python
# ä½¿ç”¨ AMP (Automatic Mixed Precision)
scaler = torch.cuda.amp.GradScaler()
```

å¯èŠ‚çœæ˜¾å­˜å¹¶åŠ é€Ÿè®­ç»ƒï¼ˆçº¦ 1.5-2 å€ï¼‰ã€‚

### 4. æ¢¯åº¦ç´¯ç§¯

å¦‚æœæ˜¾å­˜ä¸è¶³ï¼š

```python
# æ¯ 4 æ­¥ç´¯ç§¯æ¢¯åº¦åæ›´æ–°ä¸€æ¬¡
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model(batch)
    loss = loss / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

## è°ƒè¯•æŠ€å·§

### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—

```bash
export NCCL_DEBUG=INFO
export TORCH_DISTRIBUTED_DEBUG=INFO
./launch_train.sh 4
```

### 2. å•å¡æµ‹è¯•

å…ˆç”¨å•å¡ç¡®è®¤ä»£ç æ­£ç¡®ï¼š

```bash
python main_train_vrt.py --opt config.json
```

ç¡®è®¤æ— è¯¯åå†ç”¨å¤šå¡ã€‚

### 3. å°è§„æ¨¡æµ‹è¯•

ä¿®æ”¹é…ç½®ç”¨å°‘é‡æ•°æ®æµ‹è¯•ï¼š

```json
{
  "datasets": {
    "train": {
      "dataloader_batch_size": 1
    }
  },
  "train": {
    "total_iter": 100  // åªè®­ç»ƒ 100 æ­¥
  }
}
```

### 4. æ£€æŸ¥åŒæ­¥ç‚¹

åœ¨å…³é”®ä½ç½®æ·»åŠ åŒæ­¥ï¼š

```python
from utils.utils_dist import barrier

# ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åˆ°è¾¾æ­¤å¤„
barrier()
print(f"Rank {rank} passed checkpoint")
```

---

## å‚è€ƒèµ„æ–™

### PyTorch å®˜æ–¹æ–‡æ¡£

- [åˆ†å¸ƒå¼è®­ç»ƒæ•™ç¨‹](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [DistributedDataParallel API](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
- [torchrun ä½¿ç”¨æŒ‡å—](https://pytorch.org/docs/stable/elastic/run.html)

### NCCL æ–‡æ¡£

- [NCCL ç¯å¢ƒå˜é‡](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html)
- [NCCL æ€§èƒ½è°ƒä¼˜](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/tuning.html)

---

## å˜æ›´å†å²

### v2.0 (2025-11-06)

- âœ… å®Œå…¨é‡å†™åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
- âœ… æ”¯æŒ `env://` åˆå§‹åŒ–æ–¹å¼
- âœ… è‡ªåŠ¨æ£€æµ‹å¹³å° DDP å’Œæœ¬åœ°è®­ç»ƒ
- âœ… æ·»åŠ  `DistributedSampler` æ”¯æŒï¼ˆè®­ç»ƒå’ŒéªŒè¯ï¼‰
- âœ… ä¿®å¤ `LOCAL_RANK` è®¾å¤‡åˆ†é…é—®é¢˜
- âœ… å…¼å®¹ SLURM é›†ç¾¤ç¯å¢ƒ
- âœ… æ·»åŠ å®ç”¨å·¥å…·å‡½æ•°ï¼ˆ`is_main_process()`, `barrier()` ç­‰ï¼‰
- âœ… éªŒè¯/æµ‹è¯•æ—¶è‡ªåŠ¨èšåˆæ‰€æœ‰ GPU çš„æŒ‡æ ‡
- âœ… è®­ç»ƒå’ŒéªŒè¯çš„æ‰¹æ¬¡å¤§å°ã€å·¥ä½œè¿›ç¨‹æ•°è‡ªåŠ¨æŒ‰ GPU æ•°é‡åˆ†é…
- âœ… æ¨¡å‹ä¿å­˜ä½¿ç”¨åŸå­å†™å…¥å’Œä¸»è¿›ç¨‹æ£€æŸ¥

### v1.0 (Legacy)

- âš ï¸ æ—§ç‰ˆä½¿ç”¨ `torch.distributed.launch`ï¼ˆå·²å¼ƒç”¨ï¼‰
- âš ï¸ æ‰‹åŠ¨è®¾ç½® `CUDA_VISIBLE_DEVICES`ï¼ˆä¸å…¼å®¹ DDPï¼‰
- âš ï¸ ç¼ºå°‘ `DistributedSampler`ï¼ˆæ•°æ®æœªæ­£ç¡®åˆ†ç‰‡ï¼‰

---

## è”ç³»ä¸æ”¯æŒ

å¦‚é‡é—®é¢˜ï¼Œè¯·ï¼š

1. æ£€æŸ¥æ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯
2. å¯ç”¨ `NCCL_DEBUG=INFO` æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
3. å‚è€ƒæœ¬æ–‡æ¡£çš„"å¸¸è§é—®é¢˜"éƒ¨åˆ†
4. åœ¨é¡¹ç›® GitHub æäº¤ Issue

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€**

