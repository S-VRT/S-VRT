# VRT åˆ†å¸ƒå¼è®­ç»ƒæŒ‡å—

## æ¦‚è¿°

æœ¬é¡¹ç›®å·²å®Œæˆç°ä»£åŒ–åˆ†å¸ƒå¼è®­ç»ƒæ”¹é€ ï¼Œæ”¯æŒä»¥ä¸‹ä¸¤ç§åœºæ™¯ï¼š

1. **å¹³å° DDP è®­ç»ƒ**ï¼šäº‘å¹³å°/é›†ç¾¤è‡ªåŠ¨æ³¨å…¥ç¯å¢ƒå˜é‡
2. **æœ¬åœ°å¤šå¡è®­ç»ƒ**ï¼šä½¿ç”¨ `torchrun` æˆ–å•å¡è®­ç»ƒ

æ ¸å¿ƒç‰¹æ€§ï¼š
- âœ… è‡ªåŠ¨æ£€æµ‹åˆ†å¸ƒå¼æ¨¡å¼ï¼ˆåŸºäº `WORLD_SIZE` ç¯å¢ƒå˜é‡ï¼‰
- âœ… æ”¯æŒ PyTorch åŸç”Ÿ `env://` åˆå§‹åŒ–æ–¹å¼
- âœ… å…¼å®¹ SLURM é›†ç¾¤ç¯å¢ƒ
- âœ… æ­£ç¡®çš„è®¾å¤‡åˆ†é…ï¼ˆåŸºäº `LOCAL_RANK`ï¼‰
- âœ… è‡ªåŠ¨æ•°æ®åˆ†ç‰‡ï¼ˆDistributedSamplerï¼‰
- âœ… ä»…åœ¨ä¸»è¿›ç¨‹ä¿å­˜æ¨¡å‹å’Œæ—¥å¿—

---

## å¿«é€Ÿå¼€å§‹

### åœºæ™¯ä¸€ï¼šå¹³å° DDP è®­ç»ƒ

**é€‚ç”¨æƒ…å†µ**ï¼šäº‘å¹³å°ï¼ˆå¦‚é˜¿é‡Œäº‘ã€è…¾è®¯äº‘ç­‰ï¼‰å·²ä¸ºæ¯ä¸ªè¿›ç¨‹è‡ªåŠ¨æ³¨å…¥ç¯å¢ƒå˜é‡ã€‚

å¹³å°ä¼šè®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š
- `RANK`: å…¨å±€è¿›ç¨‹åºå·
- `LOCAL_RANK`: èŠ‚ç‚¹å†…è¿›ç¨‹åºå·
- `WORLD_SIZE`: æ€»è¿›ç¨‹æ•°
- `MASTER_ADDR`: ä¸»èŠ‚ç‚¹åœ°å€
- `MASTER_PORT`: ä¸»èŠ‚ç‚¹ç«¯å£

**è¿è¡Œå‘½ä»¤**ï¼š

```bash
python -u main_train_vrt.py --opt options/vrt/006_train_vrt_videodeblurring_gopro_rgbspike.json
```

âš ï¸ **é‡è¦**ï¼šä¸è¦ä½¿ç”¨ `torchrun`ï¼å¹³å°å·²ç»ä¸ºæ¯ä¸ªè¿›ç¨‹è¿è¡Œäº†ç›¸åŒçš„å‘½ä»¤ï¼Œä½¿ç”¨ `torchrun` ä¼šå¯¼è‡´åµŒå¥—å¤šè¿›ç¨‹ã€‚

---

### åœºæ™¯äºŒï¼šæœ¬åœ°å¤šå¡è®­ç»ƒ

**é€‚ç”¨æƒ…å†µ**ï¼šåœ¨æœ¬åœ°æœºå™¨æˆ–æœªé…ç½®ç¯å¢ƒå˜é‡çš„æœåŠ¡å™¨ä¸Šè®­ç»ƒã€‚

#### æ–¹å¼ 1ï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# å•å¡è®­ç»ƒ
./launch_train.sh 1

# 4 å¡è®­ç»ƒ
./launch_train.sh 4

# 8 å¡è®­ç»ƒï¼Œä½¿ç”¨è‡ªå®šä¹‰é…ç½®
./launch_train.sh 8 options/vrt/006_train_vrt_videodeblurring_gopro_rgbspike.json
```

#### æ–¹å¼ 2ï¼šç›´æ¥ä½¿ç”¨ torchrun

```bash
# 4 å¡è®­ç»ƒ
torchrun --nproc_per_node=4 main_train_vrt.py \
    --opt options/vrt/006_train_vrt_videodeblurring_gopro_rgbspike.json

# 8 å¡è®­ç»ƒ
torchrun --nproc_per_node=8 main_train_vrt.py \
    --opt options/vrt/006_train_vrt_videodeblurring_gopro_rgbspike.json
```

#### æ–¹å¼ 3ï¼šå•å¡è®­ç»ƒ

```bash
python main_train_vrt.py --opt options/vrt/006_train_vrt_videodeblurring_gopro_rgbspike.json
```

---

## é…ç½®æ–‡ä»¶è¯´æ˜

### åˆ†å¸ƒå¼ç›¸å…³é…ç½®é¡¹

åœ¨ JSON é…ç½®æ–‡ä»¶ä¸­ï¼ˆå¦‚ `006_train_vrt_videodeblurring_gopro_rgbspike.json`ï¼‰ï¼š

```json
{
  "gpu_ids": [0,1,2],
  "dist": true,
  "find_unused_parameters": false,
  "use_static_graph": true
}
```

#### `gpu_ids` å­—æ®µ

- **å•è¿›ç¨‹æ¨¡å¼**ï¼šæŒ‡å®šä½¿ç”¨å“ªäº› GPUï¼ˆä¾‹å¦‚ `[0,1,2]` è¡¨ç¤ºä½¿ç”¨ 0ã€1ã€2 å·å¡ï¼‰
- **åˆ†å¸ƒå¼æ¨¡å¼**ï¼šæ­¤å­—æ®µè¢«å¿½ç•¥ï¼Œè®¾å¤‡ç”± `torchrun` æˆ–å¹³å°è‡ªåŠ¨åˆ†é…
- **å•å¡è®­ç»ƒ**ï¼šè®¾ç½®ä¸º `[0]`

ç¨‹åºä¼šè‡ªåŠ¨è®¾ç½® `CUDA_VISIBLE_DEVICES` ç¯å¢ƒå˜é‡ï¼ˆä»…åœ¨å•è¿›ç¨‹æ¨¡å¼ä¸‹ï¼‰ã€‚

#### `dist` å­—æ®µ

- è‡ªåŠ¨ä» `WORLD_SIZE` ç¯å¢ƒå˜é‡æ£€æµ‹
- æ‰‹åŠ¨è®¾ç½®ä¼šè¢«è‡ªåŠ¨æ£€æµ‹ç»“æœè¦†ç›–
- ä¿ç•™æ­¤å­—æ®µæ˜¯ä¸ºäº†æ–‡æ¡£ç›®çš„

#### `find_unused_parameters` å­—æ®µ

- DDP ç›¸å…³å‚æ•°ï¼Œæ§åˆ¶æ˜¯å¦æŸ¥æ‰¾æœªä½¿ç”¨çš„æ¨¡å‹å‚æ•°
- VRT æ¨¡å‹è®¾ç½®ä¸º `false` å¯æå‡æ€§èƒ½

#### `use_static_graph` å­—æ®µ

- å¯ç”¨é™æ€è®¡ç®—å›¾ä¼˜åŒ–ï¼ˆPyTorch >= 1.11ï¼‰
- å¯¹å›ºå®šç½‘ç»œç»“æ„çš„æ¨¡å‹å¯æå‡è®­ç»ƒé€Ÿåº¦

---

## ç¯å¢ƒå˜é‡

### è‡ªåŠ¨è®¾ç½®çš„ç¯å¢ƒå˜é‡

åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œä»¥ä¸‹ç¯å¢ƒå˜é‡ç”± `torchrun` æˆ–å¹³å°è‡ªåŠ¨è®¾ç½®ï¼š

| ç¯å¢ƒå˜é‡ | è¯´æ˜ | ç¤ºä¾‹å€¼ |
|---------|------|--------|
| `RANK` | å…¨å±€è¿›ç¨‹åºå·ï¼ˆ0 åˆ° world_size-1ï¼‰ | `0`, `1`, `2`, ... |
| `LOCAL_RANK` | å•èŠ‚ç‚¹å†…è¿›ç¨‹åºå· | `0`, `1`, `2`, ... |
| `WORLD_SIZE` | æ€»è¿›ç¨‹æ•°ï¼ˆç­‰äº GPU æ•°é‡ï¼‰ | `4`, `8` |
| `MASTER_ADDR` | ä¸»èŠ‚ç‚¹åœ°å€ | `localhost`, `192.168.1.100` |
| `MASTER_PORT` | ä¸»èŠ‚ç‚¹ç«¯å£ | `29500` |

### æ¨èçš„ NCCL ç¯å¢ƒå˜é‡

ä¸ºäº†æå‡è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ï¼Œå»ºè®®è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

```bash
# å¼‚æ­¥é”™è¯¯å¤„ç†ï¼ˆæ¨èï¼‰
export NCCL_ASYNC_ERROR_HANDLING=1

# å¦‚æœæ²¡æœ‰ InfiniBand ç½‘ç»œï¼Œç¦ç”¨ IB
export NCCL_IB_DISABLE=1

# é™åˆ¶ CUDA è¿æ¥æ•°ï¼ˆæŸäº›æ¨¡å‹å¯æå‡ç¨³å®šæ€§ï¼‰
export CUDA_DEVICE_MAX_CONNECTIONS=1

# å¯ç”¨ NCCL è°ƒè¯•ä¿¡æ¯ï¼ˆè°ƒè¯•æ—¶ä½¿ç”¨ï¼‰
# export NCCL_DEBUG=INFO
```

å¯ä»¥å°†è¿™äº›å˜é‡æ·»åŠ åˆ° `~/.bashrc` æˆ–åœ¨è¿è¡Œè®­ç»ƒå‰å¯¼å‡ºï¼š

```bash
export NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_IB_DISABLE=1
./launch_train.sh 4
```

---

## å·¥ä½œåŸç†

### è‡ªåŠ¨æ£€æµ‹æœºåˆ¶

ç¨‹åºå¯åŠ¨æ—¶æŒ‰ä»¥ä¸‹é¡ºåºæ£€æµ‹åˆ†å¸ƒå¼æ¨¡å¼ï¼š

1. æ£€æŸ¥ `WORLD_SIZE` ç¯å¢ƒå˜é‡
   - å¦‚æœ `WORLD_SIZE > 1`ï¼šå¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
   - å¦‚æœ `WORLD_SIZE` ä¸å­˜åœ¨æˆ–ç­‰äº 1ï¼šå•è¿›ç¨‹æ¨¡å¼

2. è¯»å– rank ä¿¡æ¯ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š
   - `LOCAL_RANK` ç¯å¢ƒå˜é‡ï¼ˆtorchrun æ ‡å‡†ï¼‰
   - `SLURM_LOCALID`ï¼ˆSLURM é›†ç¾¤ï¼‰
   - é»˜è®¤å€¼ï¼š0

3. è®¾ç½® CUDA è®¾å¤‡ï¼š
   ```python
   torch.cuda.set_device(local_rank)
   device = torch.device(f"cuda:{local_rank}")
   ```

4. åˆå§‹åŒ–è¿›ç¨‹ç»„ï¼š
   ```python
   torch.distributed.init_process_group(
       backend='nccl',
       init_method='env://'
   )
   ```

### æ•°æ®åŠ è½½

åœ¨åˆ†å¸ƒå¼æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨ `DistributedSampler` è‡ªåŠ¨åˆ†ç‰‡æ•°æ®ï¼š

```python
from torch.utils.data.distributed import DistributedSampler

# åˆ›å»º sampler
sampler = DistributedSampler(
    dataset,
    num_replicas=world_size,
    rank=rank,
    shuffle=True
)

# åˆ›å»º DataLoader
dataloader = DataLoader(
    dataset,
    batch_size=batch_size,
    sampler=sampler,
    num_workers=num_workers
)

# æ¯ä¸ª epoch å¼€å§‹æ—¶è®¾ç½® epochï¼ˆç¡®ä¿æ•°æ®éšæœºæ€§ï¼‰
for epoch in range(num_epochs):
    sampler.set_epoch(epoch)
    for batch in dataloader:
        # è®­ç»ƒä»£ç 
        ...
```

### æ¨¡å‹å°è£…

æ¨¡å‹è‡ªåŠ¨ä½¿ç”¨ `DistributedDataParallel` å°è£…ï¼š

```python
model = DistributedDataParallel(
    model,
    device_ids=[local_rank],
    output_device=local_rank,
    broadcast_buffers=False,  # æ€§èƒ½ä¼˜åŒ–
    find_unused_parameters=opt['find_unused_parameters']
)
```

### æ—¥å¿—å’Œä¿å­˜

**é‡è¦åŸåˆ™**ï¼šåªåœ¨ä¸»è¿›ç¨‹ï¼ˆrank 0ï¼‰æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

- âœ… åˆ›å»ºæ—¥å¿—ç›®å½•
- âœ… ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
- âœ… ä¿å­˜è®­ç»ƒçŠ¶æ€
- âœ… å†™å…¥ TensorBoard/W&B æ—¥å¿—
- âœ… æ‰“å°è®­ç»ƒä¿¡æ¯

ä»£ç ç¤ºä¾‹ï¼š

```python
from utils.utils_dist import is_main_process

if is_main_process():
    logger.info(f"Epoch {epoch}, Loss: {loss:.4f}")
    torch.save(model.state_dict(), 'checkpoint.pth')
```

---

## å¸¸è§é—®é¢˜

### 1. ä¸ºä»€ä¹ˆä¸èƒ½åœ¨å¹³å° DDP ä¸­ä½¿ç”¨ torchrunï¼Ÿ

**åŸå› **ï¼šå¹³å°å·²ç»ä¸ºæ¯ä¸ª GPU å¯åŠ¨äº†ä¸€ä¸ªç‹¬ç«‹çš„è¿›ç¨‹ï¼Œå¹¶ä¸ºæ¯ä¸ªè¿›ç¨‹è®¾ç½®äº†æ­£ç¡®çš„ç¯å¢ƒå˜é‡ã€‚å¦‚æœå†ä½¿ç”¨ `torchrun`ï¼Œä¼šå¯¼è‡´æ¯ä¸ªè¿›ç¨‹åˆåˆ›å»ºå¤šä¸ªå­è¿›ç¨‹ï¼Œé€ æˆæ··ä¹±ã€‚

**æ­£ç¡®åšæ³•**ï¼š
```bash
# å¹³å° DDPï¼ˆæ¯ä¸ªè¿›ç¨‹è¿è¡Œç›¸åŒå‘½ä»¤ï¼‰
python -u main_train_vrt.py --opt config.json
```

**é”™è¯¯åšæ³•**ï¼š
```bash
# âŒ ä¼šå¯¼è‡´åµŒå¥—å¤šè¿›ç¨‹
torchrun --nproc_per_node=4 main_train_vrt.py --opt config.json
```

### 2. å¦‚ä½•ç¡®è®¤åˆ†å¸ƒå¼è®­ç»ƒæ­£å¸¸å·¥ä½œï¼Ÿ

æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼Œåº”è¯¥çœ‹åˆ°ï¼š

```
========================================
Distributed Training Setup
========================================
Backend: nccl
World Size: 4
Rank: 0
Local Rank: 0
Master: localhost:29500
========================================
```

æ¯ä¸ªè¿›ç¨‹ä¼šè¾“å‡ºè‡ªå·±çš„ rank ä¿¡æ¯ã€‚åªæœ‰ rank 0 ä¼šä¿å­˜æ¨¡å‹å’Œè¾“å‡ºè¯¦ç»†æ—¥å¿—ã€‚

### 3. CUDA out of memory é”™è¯¯

**åŸå› **ï¼šæ‰¹æ¬¡å¤§å°ï¼ˆbatch sizeï¼‰éœ€è¦æ ¹æ® GPU æ•°é‡è°ƒæ•´ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

- åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œæ¯å¼ å¡çš„æ‰¹æ¬¡å¤§å° = é…ç½®ä¸­çš„ `dataloader_batch_size`
- æ€»æ‰¹æ¬¡å¤§å° = `dataloader_batch_size Ã— å¡æ•°`

ä¾‹å¦‚ï¼š
- å•å¡è®­ç»ƒï¼š`batch_size = 8`
- 4 å¡è®­ç»ƒï¼šæ¯å¡ `batch_size = 2`ï¼ˆæ€»æ‰¹æ¬¡ = 2 Ã— 4 = 8ï¼‰

åœ¨é…ç½®æ–‡ä»¶ä¸­è°ƒæ•´ï¼š

```json
{
  "datasets": {
    "train": {
      "dataloader_batch_size": 2  // å‡å°æ¯å¡æ‰¹æ¬¡å¤§å°
    }
  }
}
```

### 4. NCCL åˆå§‹åŒ–è¶…æ—¶

**å¸¸è§åŸå› **ï¼š

1. é˜²ç«å¢™é˜»æ­¢è¿›ç¨‹é—´é€šä¿¡
2. `MASTER_ADDR` æˆ– `MASTER_PORT` è®¾ç½®é”™è¯¯
3. ç½‘ç»œé…ç½®é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# å¢åŠ è¶…æ—¶æ—¶é—´
export NCCL_TIMEOUT=1800

# å¯ç”¨è°ƒè¯•ä¿¡æ¯
export NCCL_DEBUG=INFO

# æŒ‡å®šç½‘ç»œæ¥å£ï¼ˆå¦‚æœæœ‰å¤šä¸ªç½‘å¡ï¼‰
export NCCL_SOCKET_IFNAME=eth0

# é‡æ–°è¿è¡Œè®­ç»ƒ
./launch_train.sh 4
```

### 5. ä¸åŒè¿›ç¨‹çš„æŸå¤±å€¼ä¸åŒæ­¥

è¿™æ˜¯æ­£å¸¸ç°è±¡ï¼æ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸åŒçš„æ•°æ®åˆ†ç‰‡ï¼Œå› æ­¤ï¼š

- âœ… æ¯ä¸ª GPU çš„æŸå¤±å€¼å¯èƒ½ä¸åŒï¼ˆæ•°æ®ä¸åŒï¼‰
- âœ… æ¢¯åº¦ä¼šåœ¨åå‘ä¼ æ’­æ—¶è‡ªåŠ¨åŒæ­¥
- âœ… æ¨¡å‹å‚æ•°åœ¨æ‰€æœ‰ GPU ä¸Šä¿æŒä¸€è‡´

å¦‚éœ€åŒæ­¥æŒ‡æ ‡ç”¨äºæ—¥å¿—è®°å½•ï¼Œä½¿ç”¨ï¼š

```python
from utils.utils_dist import all_reduce_mean

# è®¡ç®—æ‰€æœ‰è¿›ç¨‹çš„å¹³å‡æŸå¤±
avg_loss = all_reduce_mean(loss_tensor)
```

### 6. è®­ç»ƒé€Ÿåº¦æ²¡æœ‰çº¿æ€§æå‡

**æ­£å¸¸æƒ…å†µ**ï¼š

- 2 å¡ç†è®ºä¸Šåº”è¯¥å¿« 2 å€ï¼Œä½†å®é™…çº¦ 1.7-1.9 å€
- 4 å¡ç†è®ºä¸Šåº”è¯¥å¿« 4 å€ï¼Œä½†å®é™…çº¦ 3.2-3.6 å€
- 8 å¡ç†è®ºä¸Šåº”è¯¥å¿« 8 å€ï¼Œä½†å®é™…çº¦ 6-7 å€

**åŸå› **ï¼š

1. é€šä¿¡å¼€é”€ï¼ˆæ¢¯åº¦åŒæ­¥ï¼‰
2. I/O ç“¶é¢ˆï¼ˆæ•°æ®åŠ è½½ï¼‰
3. è´Ÿè½½ä¸å‡è¡¡

**ä¼˜åŒ–å»ºè®®**ï¼š

```json
{
  "datasets": {
    "train": {
      "dataloader_num_workers": 8,  // å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹
      "dataloader_batch_size": 4    // å¢å¤§æ‰¹æ¬¡å‡å°‘é€šä¿¡é¢‘ç‡
    }
  }
}
```

---

## SLURM é›†ç¾¤æ”¯æŒ

æœ¬é¡¹ç›®å…¼å®¹ SLURM ä½œä¸šè°ƒåº¦ç³»ç»Ÿã€‚

### SLURM ä»»åŠ¡è„šæœ¬ç¤ºä¾‹

åˆ›å»º `submit_job.sh`ï¼š

```bash
#!/bin/bash
#SBATCH --job-name=vrt_train
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=4
#SBATCH --gres=gpu:4
#SBATCH --time=48:00:00
#SBATCH --output=logs/slurm_%j.out
#SBATCH --error=logs/slurm_%j.err

# åŠ è½½ç¯å¢ƒ
module load cuda/11.8
module load pytorch/2.0

# NCCL é…ç½®
export NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_IB_DISABLE=1

# è¿è¡Œè®­ç»ƒï¼ˆSLURM ä¼šè‡ªåŠ¨è®¾ç½®ç¯å¢ƒå˜é‡ï¼‰
srun python -u main_train_vrt.py \
    --opt options/vrt/006_train_vrt_videodeblurring_gopro_rgbspike.json
```

æäº¤ä»»åŠ¡ï¼š

```bash
sbatch submit_job.sh
```

SLURM ä¼šè‡ªåŠ¨è®¾ç½®ï¼š
- `SLURM_PROCID` â†’ æ˜ å°„åˆ° `RANK`
- `SLURM_LOCALID` â†’ æ˜ å°„åˆ° `LOCAL_RANK`
- `SLURM_NTASKS` â†’ æ˜ å°„åˆ° `WORLD_SIZE`

---

## ç›‘æ§è®­ç»ƒ

### TensorBoard

è®­ç»ƒæ—¥å¿—è‡ªåŠ¨ä¿å­˜åˆ° `experiments/<task_name>/tb_logger/`ï¼š

```bash
tensorboard --logdir experiments/006_train_vrt_videodeblurring_gopro_rgbspike/tb_logger
```

### Weights & Biases

å¦‚æœé…ç½®äº† W&Bï¼š

```json
{
  "logging": {
    "use_wandb": true,
    "wandb_project": "VRT-VideoDeblurring",
    "wandb_api_key": "your_api_key"
  }
}
```

è®­ç»ƒä¼šè‡ªåŠ¨ä¸Šä¼ åˆ° W&B å¹³å°ã€‚

### å‘½ä»¤è¡Œè¾“å‡º

åªæœ‰ä¸»è¿›ç¨‹ï¼ˆrank 0ï¼‰ä¼šè¾“å‡ºè¯¦ç»†ä¿¡æ¯ï¼š

```
[2025-11-06 10:00:00] Epoch: 1, Iter: 100, Loss: 0.0234
[2025-11-06 10:05:00] Epoch: 1, Iter: 200, Loss: 0.0198
```

å…¶ä»–è¿›ç¨‹ä¼šä¿æŒé™é»˜æˆ–åªè¾“å‡ºå…³é”®ä¿¡æ¯ã€‚

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ‰¹æ¬¡å¤§å°è°ƒä¼˜

```json
{
  "datasets": {
    "train": {
      "dataloader_batch_size": 4  // æ¯å¡æ‰¹æ¬¡ï¼Œæ ¹æ®æ˜¾å­˜è°ƒæ•´
    }
  }
}
```

**å»ºè®®**ï¼š
- A100 80GB: batch_size = 8-16
- V100 32GB: batch_size = 4-8
- RTX 3090 24GB: batch_size = 2-4

### 2. æ•°æ®åŠ è½½ä¼˜åŒ–

```json
{
  "datasets": {
    "train": {
      "dataloader_num_workers": 8  // CPU æ ¸å¿ƒæ•°çš„ä¸€åŠ
    }
  }
}
```

### 3. æ··åˆç²¾åº¦è®­ç»ƒ

åœ¨ `utils_option.py` ä¸­å¯ç”¨ï¼š

```python
# ä½¿ç”¨ AMP (Automatic Mixed Precision)
scaler = torch.cuda.amp.GradScaler()
```

å¯èŠ‚çœæ˜¾å­˜å¹¶åŠ é€Ÿè®­ç»ƒï¼ˆçº¦ 1.5-2 å€ï¼‰ã€‚

### 4. æ¢¯åº¦ç´¯ç§¯

å¦‚æœæ˜¾å­˜ä¸è¶³ï¼š

```python
# æ¯ 4 æ­¥ç´¯ç§¯æ¢¯åº¦åæ›´æ–°ä¸€æ¬¡
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model(batch)
    loss = loss / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

## è°ƒè¯•æŠ€å·§

### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—

```bash
export NCCL_DEBUG=INFO
export TORCH_DISTRIBUTED_DEBUG=INFO
./launch_train.sh 4
```

### 2. å•å¡æµ‹è¯•

å…ˆç”¨å•å¡ç¡®è®¤ä»£ç æ­£ç¡®ï¼š

```bash
python main_train_vrt.py --opt config.json
```

ç¡®è®¤æ— è¯¯åå†ç”¨å¤šå¡ã€‚

### 3. å°è§„æ¨¡æµ‹è¯•

ä¿®æ”¹é…ç½®ç”¨å°‘é‡æ•°æ®æµ‹è¯•ï¼š

```json
{
  "datasets": {
    "train": {
      "dataloader_batch_size": 1
    }
  },
  "train": {
    "total_iter": 100  // åªè®­ç»ƒ 100 æ­¥
  }
}
```

### 4. æ£€æŸ¥åŒæ­¥ç‚¹

åœ¨å…³é”®ä½ç½®æ·»åŠ åŒæ­¥ï¼š

```python
from utils.utils_dist import barrier

# ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åˆ°è¾¾æ­¤å¤„
barrier()
print(f"Rank {rank} passed checkpoint")
```

---

## å‚è€ƒèµ„æ–™

### PyTorch å®˜æ–¹æ–‡æ¡£

- [åˆ†å¸ƒå¼è®­ç»ƒæ•™ç¨‹](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [DistributedDataParallel API](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
- [torchrun ä½¿ç”¨æŒ‡å—](https://pytorch.org/docs/stable/elastic/run.html)

### NCCL æ–‡æ¡£

- [NCCL ç¯å¢ƒå˜é‡](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html)
- [NCCL æ€§èƒ½è°ƒä¼˜](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/tuning.html)

---

## å˜æ›´å†å²

### v2.0 (2025-11-06)

- âœ… å®Œå…¨é‡å†™åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
- âœ… æ”¯æŒ `env://` åˆå§‹åŒ–æ–¹å¼
- âœ… è‡ªåŠ¨æ£€æµ‹å¹³å° DDP å’Œæœ¬åœ°è®­ç»ƒ
- âœ… æ·»åŠ  `DistributedSampler` æ”¯æŒ
- âœ… ä¿®å¤ `LOCAL_RANK` è®¾å¤‡åˆ†é…é—®é¢˜
- âœ… å…¼å®¹ SLURM é›†ç¾¤ç¯å¢ƒ
- âœ… æ·»åŠ å®ç”¨å·¥å…·å‡½æ•°ï¼ˆ`is_main_process()`, `barrier()` ç­‰ï¼‰

### v1.0 (Legacy)

- âš ï¸ æ—§ç‰ˆä½¿ç”¨ `torch.distributed.launch`ï¼ˆå·²å¼ƒç”¨ï¼‰
- âš ï¸ æ‰‹åŠ¨è®¾ç½® `CUDA_VISIBLE_DEVICES`ï¼ˆä¸å…¼å®¹ DDPï¼‰
- âš ï¸ ç¼ºå°‘ `DistributedSampler`ï¼ˆæ•°æ®æœªæ­£ç¡®åˆ†ç‰‡ï¼‰

---

## è”ç³»ä¸æ”¯æŒ

å¦‚é‡é—®é¢˜ï¼Œè¯·ï¼š

1. æ£€æŸ¥æ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯
2. å¯ç”¨ `NCCL_DEBUG=INFO` æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
3. å‚è€ƒæœ¬æ–‡æ¡£çš„"å¸¸è§é—®é¢˜"éƒ¨åˆ†
4. åœ¨é¡¹ç›® GitHub æäº¤ Issue

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€**

