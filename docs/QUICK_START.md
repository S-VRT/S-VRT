# Quick Start - Optimized Training

## ğŸš€ Run Training (3 GPUs)

```bash
cd /home/mallm/henry/Deblur
bash scripts/launch_train.sh
```

**Expected performance**: â‰¥2.0 samples/s (5Ã— faster than baseline)

---

## ğŸ“Š Run Profiling (1 GPU, 100 steps)

```bash
bash scripts/profile_train.sh
```

View results:
```bash
tensorboard --logdir=outputs/prof/
```

---

## ğŸ“ˆ Monitor Training

### Real-time Metrics
```bash
# Terminal 1: Training logs
tail -f outputs/logs/train_*.log

# Terminal 2: GPU monitoring
watch nvidia-smi

# Terminal 3: TensorBoard
tensorboard --logdir=outputs/logs/tb
```

### Key Metrics
- **samples/s**: Target â‰¥2.0 (shown in logs every 50 steps)
- **GPU util**: Target >90% (nvidia-smi)
- **data_time_ms**: Target <10 (shown in logs)
- **loss**: Should decrease smoothly (TensorBoard)

---

## âš™ï¸ Configuration

**File**: `configs/deblur/vrt_spike_baseline.yaml`

### Key Settings

```yaml
DATA:
  USE_RAM_CACHE: true  # âš¡ 2-3Ã— speedup (needs ~15 GB RAM)

TRAIN:
  COMPILE_MODEL: true  # âš¡ 1.2Ã— speedup (PyTorch 2.0+)
  BATCH_SIZE: 1
  GRADIENT_ACCUMULATION_STEPS: 12  # Effective batch = 36
  NUM_WORKERS: 36  # 12 per GPU
  PREFETCH_FACTOR: 4
```

---

## ğŸ› ï¸ Troubleshooting

### Out of Memory?

**System RAM**:
```yaml
DATA:
  USE_RAM_CACHE: false  # Disable caching
```

**GPU VRAM**:
```yaml
TRAIN:
  NUM_WORKERS: 12  # Reduce from 36
```

### torch.compile errors?

```yaml
TRAIN:
  COMPILE_MODEL: false  # Disable compilation
```

### Low throughput?

1. Check data_time_ms in logs (should be <10)
2. Verify GPU util >90% with `nvidia-smi`
3. Run profiler: `bash scripts/profile_train.sh`

---

## ğŸ“š Documentation

- **Full guide**: `docs/TRAINING_OPTIMIZATION_GUIDE.md`
- **Testing**: `docs/OPTIMIZATION_TESTING_CHECKLIST.md`
- **Summary**: `OPTIMIZATION_SUMMARY.md`

---

## ğŸ¯ Expected Results

| Metric | Before | After |
|--------|--------|-------|
| Throughput | 0.4/s | â‰¥2.0/s |
| GPU util | 60-70% | >90% |
| Data time | ~100ms | <10ms |
| Batch time | ~2.5s | <500ms |

---

## âœ… Quick Validation

```bash
# 1. Start training
bash scripts/launch_train.sh

# 2. Check logs after ~100 steps
tail outputs/logs/train_*.log

# 3. Look for:
# "2.3 samples/s | data_time=4.2ms"
#       ^^^              ^^^
#    Target â‰¥2.0      Target <10
```

**If both targets met**: âœ… Optimization successful!

---

## ğŸ”§ Advanced

### Single GPU training:
```bash
# Edit scripts/launch_train.sh
NUM_GPUS=1
```

### Profile specific steps:
```bash
python src/train.py \
  --config configs/deblur/vrt_spike_baseline.yaml \
  --profile --profile_steps 200
```

### Resume from checkpoint:
```bash
# (Feature to be added if needed)
```

---

## ğŸ“š Additional Resources

### Memory Optimization

å¦‚æœé‡åˆ°OOMï¼ˆå†…å­˜ä¸è¶³ï¼‰é”™è¯¯ï¼Œè¯·å‚é˜…ï¼š
- **[MEMORY_OPTIMIZATION.md](MEMORY_OPTIMIZATION.md)** - è¯¦ç»†çš„å†…å­˜ä¼˜åŒ–æŒ‡å—
  - LRUç¼“å­˜ç­–ç•¥è¯´æ˜
  - Workeræ•°é‡è°ƒä¼˜
  - å†…å­˜ç›‘æ§å·¥å…·
  - æ•…éšœæ’é™¤æŒ‡å—

**å¿«é€Ÿä¿®å¤OOMï¼š**
```yaml
# ç¼–è¾‘ configs/deblur/vrt_spike_baseline.yaml
DATA:
  CACHE_SIZE_GB: 30.0  # é™ä½ç¼“å­˜å¤§å°ï¼ˆé»˜è®¤50GBï¼‰

TRAIN:
  NUM_WORKERS: 8       # å‡å°‘workersï¼ˆé»˜è®¤12ï¼‰
```

### Documentation

- [OPTIMIZATION_SUMMARY.md](OPTIMIZATION_SUMMARY.md) - å®Œæ•´ä¼˜åŒ–å®æ–½æ€»ç»“
- [docs/](docs/) - è¯¦ç»†æŠ€æœ¯æ–‡æ¡£

---

**Ready to train!** ğŸš€

